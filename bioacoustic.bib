# --
# bioacoustic reference collections

% marine, animal welfare, monitoring
@article{Winship2023Marine,
  author = {Winship, Kelley A. and Jones, Brittany L.},
  title = {Acoustic Monitoring of Professionally Managed Marine Mammals for Health and Welfare Insights},
  journal = {Animals},
  volume = {13},
  number = {13},
  year = {2023},
  %ARTICLE-NUMBER = {2124},
  %URL = {https://www.mdpi.com/2076-2615/13/13/2124},
  %PubMedID = {37443922},
  %ISSN = {2076-2615},
  %DOI = {10.3390/ani13132124},
  %ABSTRACT = {Research evaluating marine mammal welfare and opportunities for advancements in the care of species housed in a professional facility have rapidly increased in the past decade. While topics, such as comfortable housing, adequate social opportunities, stimulating enrichment, and a high standard of medical care, have continued to receive attention from managers and scientists, there is a lack of established acoustic consideration for monitoring the welfare of these animals. Marine mammals rely on sound production and reception for navigation and communication. Regulations governing anthropogenic sound production in our oceans have been put in place by many countries around the world, largely based on the results of research with managed and trained animals, due to the potential negative impacts that unrestricted noise can have on marine mammals. However, there has not been an established best practice for the acoustic welfare monitoring of marine mammals in professional care. By monitoring animal hearing and vocal behavior, a more holistic view of animal welfare can be achieved through the early detection of anthropogenic sound sources, the acoustic behavior of the animals, and even the features of the calls. In this review, the practice of monitoring cetacean acoustic welfare through behavioral hearing tests and auditory evoked potentials (AEPs), passive acoustic monitoring, such as the Welfare Acoustic Monitoring System (WAMS), as well as ideas for using advanced technologies for utilizing vocal biomarkers of health are introduced and reviewed as opportunities for integration into marine mammal welfare plans.},
}

% bonobo, animal communications
@article{Arnaud2023Bonobo,
  author = {Arnaud, Vincent and Pellegrino, François and Keenan, Sumir and St-Gelais, Xavier and Mathevon, Nicolas and Levréro, Florence and Coupé, Christophe},
  title = {Improving the workflow to crack Small, Unbalanced, Noisy, but Genuine (SUNG) datasets in bioacoustics: The case of bonobo calls},
  journal = {PLOS Computational Biology},
  publisher = {Public Library of Science},
  volume = {19},
  pages = {1-47},
  number = {4},
  month = {04},
  year = {2023},
  %url = {https://doi.org/10.1371/journal.pcbi.1010325},
  %doi = {10.1371/journal.pcbi.1010325},
  %abstract = {Despite the accumulation of data and studies, deciphering animal vocal communication remains challenging. In most cases, researchers must deal with the sparse recordings composing Small, Unbalanced, Noisy, but Genuine (SUNG) datasets. SUNG datasets are characterized by a limited number of recordings, most often noisy, and unbalanced in number between the individuals or categories of vocalizations. SUNG datasets therefore offer a valuable but inevitably distorted vision of communication systems. Adopting the best practices in their analysis is essential to effectively extract the available information and draw reliable conclusions. Here we show that the most recent advances in machine learning applied to a SUNG dataset succeed in unraveling the complex vocal repertoire of the bonobo, and we propose a workflow that can be effective with other animal species. We implement acoustic parameterization in three feature spaces and run a Supervised Uniform Manifold Approximation and Projection (S-UMAP) to evaluate how call types and individual signatures cluster in the bonobo acoustic space. We then implement three classification algorithms (Support Vector Machine, xgboost, neural networks) and their combination to explore the structure and variability of bonobo calls, as well as the robustness of the individual signature they encode. We underscore how classification performance is affected by the feature set and identify the most informative features. In addition, we highlight the need to address data leakage in the evaluation of classification performance to avoid misleading interpretations. Our results lead to identifying several practical approaches that are generalizable to any other animal communication system. To improve the reliability and replicability of vocal communication studies with SUNG datasets, we thus recommend: i) comparing several acoustic parameterizations; ii) visualizing the dataset with supervised UMAP to examine the species acoustic space; iii) adopting Support Vector Machines as the baseline classification approach; iv) explicitly evaluating data leakage and possibly implementing a mitigation strategy.},
}

% chicken, monitoring
@article{Mahdavian2021ChickenMonitoring,
  author = {Alireza Mahdavian and Saeid Minaei and Peter M. Marchetto and Farshad Almasganj and Shaban Rahimi and Ce Yang},
  title = {Acoustic features of vocalization signal in poultry health monitoring},
  journal = {Applied Acoustics},
  volume = {175},
  pages = {107756},
  year = {2021},
  %issn = {0003-682X},
  %doi = {https://doi.org/10.1016/j.apacoust.2020.107756},
  %url = {https://www.sciencedirect.com/science/article/pii/S0003682X20308616},
  %keywords = {Bioacoustics, Health monitoring, Respiratory diseases, Audio features, Precision livestock farming},
  %abstract = {In precision livestock farming, studies show that bird sound can be employed as a biomarker of health condition. One of the most important steps for this purpose is to study the feasibility of using acoustic features as criteria for disease diagnosis. In this research five acoustic features of bird calls were evaluated for determination of bird health condition. Signals were collected from broilers grown in three groups: control, challenged with Bronchitis, and challenged with Newcastle disease. Results of data analysis showed that, among the 5 acoustic features studied, wavelet entropy (WET) had the best performance and was able to detect Bronchitis on the third day after inoculation with 83% accuracy while the type II error in this test (incorrectly detecting sick bird as healthy) was less than 14% and 6% on the third day and fourth day, respectively. In the case of Newcastle disease, although WET and Mel cepstral coefficients (MFCC) exhibited similar accuracy (80% and 78% respectively on the fourth day), but the difference was that WET was more reliable in detecting healthy birds while MFCC had better performance detecting challenged birds.},
}

% pig, welfare
@article{Cordeiro2018PigVoc,
  author = {Alexandra F.da S. Cordeiro and Irenilza de A. Nääs and Felipe {da Silva Leitão} and Andréia C.M. {de Almeida} and Daniella Jorge {de Moura}},
  title = {Use of vocalisation to identify sex, age, and distress in pig production},
  journal = {Biosystems Engineering},
  volume = {173},
  pages = {57-63},
  year = {2018},
  %note = {Advances in the Engineering of Sensor-based Monitoring and Management Systems for Precision Livestock Farming},
  %issn = {1537-5110},
  %doi = {https://doi.org/10.1016/j.biosystemseng.2018.03.007},
  %url = {https://www.sciencedirect.com/science/article/pii/S153751101730435X},
  %keywords = {Pig welfare, Acoustic signals, Pig production, Machine learning},
  %abstract = {To assess animal welfare at a pig production farm is a time-consuming task. The present study aimed to investigate the differences in pig vocalisation as a function of the sex, age, and distress conditions, and to propose a way of identifying distressful situations. The individual vocalisations of 40 pigs were recorded (20 male and 20 female) during exposure to different distress in the farrowing, nursery, growth, and finishing phases. Vocalisation pitch differed between males (194.5 Hz) and females (218.2 Hz). Pig vocalisation was also different according to age, especially for the attributes of maximum and minimum amplitudes, and the frequency of formant 2. Diverse distress situations also were identified by various acoustic attributes. A decision-tree for classifying the distress condition for pigs was built (with an accuracy of 81.92%) using the machine-learning technique. Results indicate the possibility of estimating pig welfare by recording the vocalisation. The algorithm is also promising to identify pig sex and age.},
}

% cow, monitoring, plf
@article{Jung2021DLCow,
  author = {Jung, Dae-Hyun and Kim, Na Yeon and Moon, Sang Ho and Jhin, Changho and Kim, Hak-Jin and Yang, Jung-Seok and Kim, Hyoung Seok and Lee, Taek Sung and Lee, Ju Young and Park, Soo Hyun},
  title = {Deep Learning-Based Cattle Vocal Classification Model and Real-Time Livestock Monitoring System with Noise Filtering},
  journal = {Animals},
  volume = {11},
  year = {2021},
  number = {2},
  %article-number = {357},
  %url = {https://www.mdpi.com/2076-2615/11/2/357},
  %PubMedID = {33535390},
  %issn = {2076-2615},
  %doi = {10.3390/ani11020357},
  %abstract = {The priority placed on animal welfare in the meat industry is increasing the importance of understanding livestock behavior. In this study, we developed a web-based monitoring and recording system based on artificial intelligence analysis for the classification of cattle sounds. The deep learning classification model of the system is a convolutional neural network (CNN) model that takes voice information converted to Mel-frequency cepstral coefficients (MFCCs) as input. The CNN model first achieved an accuracy of 91.38% in recognizing cattle sounds. Further, short-time Fourier transform-based noise filtering was applied to remove background noise, improving the classification model accuracy to 94.18%. Categorized cattle voices were then classified into four classes, and a total of 897 classification records were acquired for the classification model development. A final accuracy of 81.96% was obtained for the model. Our proposed web-based platform that provides information obtained from a total of 12 sound sensors provides cattle vocalization monitoring in real time, enabling farm owners to determine the status of their cattle.},
}

% cow, monitoring, welfare
@article{Meen2015SoundAnalysisCattleWelfare,
  author = {G.H. Meen and M.A. Schellekens and M.H.M. Slegers and N.L.G. Leenders and E. {van Erp-van der Kooij} and L.P.J.J. Noldus},
  title = {Sound analysis in dairy cattle vocalisation as a potential welfare monitor},
  journal = {Computers and Electronics in Agriculture},
  volume = {118},
  pages = {111-115},
  year = {2015},
  %issn = {0168-1699},
  %doi = {https://doi.org/10.1016/j.compag.2015.08.028},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168169915002549},
  %keywords = {Precision livestock farming, Sound analysis, Dairy cattle, Behaviour, Video analysis, Audio analysis},
  %abstract = {In modern farming there is a growing demand for innovative tools gathering and analysing information concerning the herd, as well as individual animals. In Precision Livestock Farming (PLF), technology continuously measures various variables as activity, food intake or oestrus activity, thereby supporting farmers in monitoring his livestock. Sound analysis has shown to be useful as an early warning tool in pigs and it is unknown whether sound analysis can also be applied in cattle. Goal of this research was to determine whether a correlation can be found between cattle vocalisation and cattle behaviour. The vocalisations and behaviour of Holstein Friesian cattle were observed using audio and video recordings. Four cameras and four microphones were installed at a high production dairy farm in Herwijnen, the Netherlands. Three sets (a set consisting of both a camera and a microphone) recorded dairy cattle between two and fourteen years of age, one set recorded heifers between four and ten months of age. Recordings were made for fifteen days in three consecutive weeks, ten hours per day. Calls of cattle were traced to an individual cow and, if possible, linked with simultaneously expressed behaviour. The used ethogram consisted of six behavioural groups: lying & ruminating, feeding related behaviour, social interaction, sexual related behaviour, stress related behaviour and remaining behaviour. Lying & ruminating was a separate class since this behaviour expresses the needs of a cow. The maximum frequency in Hertz (Hz) of each call was determined. Statistical analysis showed a significant difference between the mean maximum frequency (Hz) of calls during lying & ruminating and calls recorded during other behaviours (83±4.3Hz versus 298±8.0Hz; p<0.05). Calls by adult dairy cattle had a significantly lower maximum frequency (Hz) than calls by heifers. (332.6±0.2Hz versus 218.5Hz±0.3Hz; p<0.05). This study may provide a foothold towards the use of sound analysis as a tool for dairy cattle management. If calls by cattle can be used to monitor welfare, dairy farmers can be alerted when cattle welfare is decreasing.},
}

% sperm whale, technical, contrastive learning
@article {Bermant2022CLWhale,
  author = {Peter C. Bermant and Leandra Brickson and Alexander J. Titus},
  title = {Bioacoustic Event Detection with Self-Supervised Contrastive Learning},
  year = {2022},
  publisher = {Cold Spring Harbor Laboratory},
  journal = {bioRxiv},
  %doi = {10.1101/2022.10.12.511740},
  %elocation-id = {2022.10.12.511740},
  %URL = {https://www.biorxiv.org/content/early/2022/10/16/2022.10.12.511740.1},
  %eprint = {https://www.biorxiv.org/content/early/2022/10/16/2022.10.12.511740.1.full.pdf},
  %abstract = {While deep learning has revolutionized ecological data analysis, existing strategies often rely on supervised learning, which is subject to limitations on real-world applicability. In this paper, we apply self-supervised deep learning methods to bioacoustic data to enable unsupervised detection of bioacoustic event boundaries. We propose a convolutional deep neural network that operates on the raw waveform directly and is trained in accordance with the Noise Contrastive Estimation principle, which enables the system to detect spectral changes in the input acoustic stream. The model learns a representation of the input audio sampled at low frequency that encodes information regarding dissimilarity between sequential acoustic windows. During inference, we use a peak finding algorithm to search for regions of high dissimilarity in order to identify temporal boundaries of bioacoustic events. We report results using these techniques to detect sperm whale (Physeter macrocephalus) coda clicks in real-world recordings, and we demonstrate the viability of analyzing the vocalizations of other species (e.g. Bengalese finch syllable segmentation) in addition to other data modalities (e.g. animal behavioral dynamics, embryo development and tracking). We find that the self-supervised deep representation learning-based technique outperforms established threshold-based baseline methods without requiring manual annotation of acoustic datasets. Quantitatively, our approach yields a maximal R-value and F1-score of 0.887 and 0.876, respectively, and an area under the Precision-Recall curve (PR-AUC) of 0.917, while a baseline threshold detector acting on signal energy amplitude returns a maximal R-value and F1-score of 0.620 and 0.576, respectively, and a PR-AUC of 0.571. We also compare with a threshold detector using preprocessed (e.g. denoised) acoustic input. The findings of this paper establish the validity of unsupervised bioacoustic event detection using deep neural networks and self-supervised contrastive learning as an effective alternative to conventional techniques that leverage supervised methods for signal presence indication. Providing a means for highly accurate unsupervised detection, this paper serves as an important step towards developing a fully automated system for real-time acoustic monitoring of bioacoustic signals in real-world acoustic data. All code and data used in this study are available online.Competing Interest StatementThe authors have declared no competing interest.},
}

% monitoring, survey
@article{Gibb2019EcologicalAssesment,
  author = {Gibb, Rory and Browning, Ella and Glover-Kapfer, Paul and Jones, Kate E.},
  title = {Emerging opportunities and challenges for passive acoustics in ecological assessment and monitoring},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {2},
  pages = {169-185},
  year = {2019},
  %keywords = {acoustic indices, bioacoustics, biodiversity monitoring, deep learning, ecoacoustics, ecological monitoring, machine learning, passive acoustic monitoring},
  %doi = {https://doi.org/10.1111/2041-210X.13101},
  %url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13101},
  %eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13101},
  %abstract = {Abstract High-throughput environmental sensing technologies are increasingly central to global monitoring of the ecological impacts of human activities. In particular, the recent boom in passive acoustic sensors has provided efficient, noninvasive, and taxonomically broad means to study wildlife populations and communities, and monitor their responses to environmental change. However, until recently, technological costs and constraints have largely confined research in passive acoustic monitoring (PAM) to a handful of taxonomic groups (e.g., bats, cetaceans, birds), often in relatively small-scale, proof-of-concept studies. The arrival of low-cost, open-source sensors is now rapidly expanding access to PAM technologies, making it vital to evaluate where these tools can contribute to broader efforts in ecology and biodiversity research. Here, we synthesise and critically assess the current emerging opportunities and challenges for PAM for ecological assessment and monitoring of both species populations and communities. We show that terrestrial and marine PAM applications are advancing rapidly, facilitated by emerging sensor hardware, the application of machine learning innovations to automated wildlife call identification, and work towards developing acoustic biodiversity indicators. However, the broader scope of PAM research remains constrained by limited availability of reference sound libraries and open-source audio processing tools, especially for the tropics, and lack of clarity around the accuracy, transferability and limitations of many analytical methods. In order to improve possibilities for PAM globally, we emphasise the need for collaborative work to develop standardised survey and analysis protocols, publicly archived sound libraries, multiyear audio datasets, and a more robust theoretical and analytical framework for monitoring vocalising animal communities.},
}

% monitoring, survey, literature review
@inproceedings{Sharma2022BioAcMonitoring,
  author={Sharma, Sandhya and Sato, Kazuhiko and Gautam, Bishnu Prasad},
  title={Bioacoustics Monitoring of Wildlife using Artificial Intelligence: A Methodological Literature Review}, 
  booktitle={2022 International Conference on Networking and Network Applications (NaNA)}, 
  year={2022},
  volume={},
  number={},
  pages={329-337},
  %keywords={Social networking (online);Biological system modeling;Bibliographies;Wildlife;Prediction algorithms;Classification algorithms;Spatiotemporal phenomena;Artificial Intelligence;bioacoustics;monitoring;review;wildlife},
  %doi={10.1109/NaNA56854.2022.00063},
}

% identify animal species
@article{Ovaskainen2018ASI,
  author = {Ovaskainen, Otso and Moliterno de Camargo, Ulisses and Somervuo, Panu},
  title = {Animal Sound Identifier (ASI): software for automated identification of vocal animals},
  journal = {Ecology Letters},
  volume = {21},
  number = {8},
  pages = {1244-1254},
  year = {2018},
  %keywords = {Automated vocal identification, autonomous audio recording, joint species distribution modelling, species classification, species identification, vocal communities},
  %doi = {https://doi.org/10.1111/ele.13092},
  %url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.13092},
  %eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ele.13092},
  %abstract = {Abstract Automated audio recording offers a powerful tool for acoustic monitoring schemes of bird, bat, frog and other vocal organisms, but the lack of automated species identification methods has made it difficult to fully utilise such data. We developed Animal Sound Identifier (ASI), a MATLAB software that performs probabilistic classification of species occurrences from field recordings. Unlike most previous approaches, ASI locates training data directly from the field recordings and thus avoids the need of pre-defined reference libraries. We apply ASI to a case study on Amazonian birds, in which we classify the vocalisations of 14 species in 194 504 one-minute audio segments using in total two weeks of expert time to construct, parameterise, and validate the classification models. We compare the classification performance of ASI (with training templates extracted automatically from field data) to that of monitoR (with training templates extracted manually from the Xeno-Canto database), the results showing ASI to have substantially higher recall and precision rates.},
}

% soundscape, biodiversity
@article{Sethi2023LimitsScBiodiv,
  author = {Sarab S. Sethi and Avery Bick and Robert M. Ewers and Holger Klinck and Vijay Ramesh and Mao Ning Tuanmu and David A. Coomes},
  title = {Limits to the accurate and generalizable use of soundscapes to monitor biodiversity},
  journal = {Nature Ecology and Evolution},
  issue = {9},
  volume = {7},
  year = {2023},
  %doi = {10.1038/s41559-023-02148-z},
  %issn = {2397334X},
  %abstract = {Although eco-acoustic monitoring has the potential to deliver biodiversity insight on vast scales, existing analytical approaches behave unpredictably across studies. We collated 8,023 audio recordings with paired manual avifaunal point counts to investigate whether soundscapes could be used to monitor biodiversity across diverse ecosystems. We found that neither univariate indices nor machine learning models were predictive of species richness across datasets but soundscape change was consistently indicative of community change. Our findings indicate that there are no common features of biodiverse soundscapes and that soundscape monitoring should be used cautiously and in conjunction with more reliable in-person ecological surveys.},
}

% soundscape, encoding
@article{Sethi2020SoundscapeFeature
  author = {Sarab S. Sethi  and Nick S. Jones  and Ben D. Fulcher  and Lorenzo Picinali  and Dena Jane Clink  and Holger Klinck  and C. David L. Orme  and Peter H. Wrege  and Robert M. Ewers },
  title = {Characterizing soundscapes across diverse ecosystems using a universal acoustic feature set},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {29},
  pages = {17049-17055},
  year = {2020},
  %doi = {10.1073/pnas.2004702117},
  %URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2004702117},
  %eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2004702117},
  %abstract = {Natural habitats are being impacted by human pressures at an alarming rate. Monitoring these ecosystem-level changes often requires labor-intensive surveys that are unable to detect rapid or unanticipated environmental changes. Here we have developed a generalizable, data-driven solution to this challenge using eco-acoustic data. We exploited a convolutional neural network to embed soundscapes from a variety of ecosystems into a common acoustic space. In both supervised and unsupervised modes, this allowed us to accurately quantify variation in habitat quality across space and in biodiversity through time. On the scale of seconds, we learned a typical soundscape model that allowed automatic identification of anomalous sounds in playback experiments, providing a potential route for real-time automated detection of irregular environmental behavior including illegal logging and hunting. Our highly generalizable approach, and the common set of features, will enable scientists to unlock previously hidden insights from acoustic data and offers promise as a backbone technology for global collaborative autonomous ecosystem monitoring efforts.},
}

% poultry, chicken
@article{Mahdavian2020PoultryCalls,
  author = {Alireza Mahdavian and Saeid Minaei and Ce Yang and Farshad Almasganj and Shaban Rahimi and Peter M. Marchetto},
  title = {Ability evaluation of a voice activity detection algorithm in bioacoustics: A case study on poultry calls},
  journal = {Computers and Electronics in Agriculture},
  volume = {168},
  pages = {105100},
  year = {2020},
  %issn = {0168-1699},
  %doi = {https://doi.org/10.1016/j.compag.2019.105100},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168169918305982},
  %keywords = {Bioacoustics, Health monitoring, Respiratory diseases, Audio features, },
  %abstract = {Poultry is one of the most strategic source of human foods. There have been seen some hopeful signs of bioacoustics application to monitor the health condition of this vital food source. One of the obstacles is that the bird’s call is combined with some unvoiced sounds and extracting the calls is not easy, especially when the bird is sick. This research is a report on successful application of some of the features involved in extracting healthy and non-healthy birds’ calls from their sound signals. One hundred and twenty birds from two genotypes – Ross and Cobb – were placed in two groups, a control and those challenged with respiratory diseases. They were reared and their sound was recorded daily. The vocal phrases of the recorded audio signals were extracted using the presented algorithm. Results of analysis showed that an increase in age and onset of illness are two factors that cause an error increase. Detection accuracy was calculated at 95% for healthy young birds and 72% for non-healthy birds. A significant part of this error is due to misclassing the calls as non-vocal segments. This meant that 97% of the activities classified as vocal phrases were, in fact, vocal. These results showed that the idea of such an easy-to-implement algorithm could potentially be employed for the coarselevel segmentation of some animal vocalization signals with reliable outputs, which is an essential and primary step in bioacoustics research.},
}

% contrastive
@inproceedings{Moummad2023CL,
  author={Ilyass Moummad and Romain Serizel and Nicolas Farrugia},
  title={Pretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning}, 
  booktitle = {Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)},
  month = {9},
  year={2023},
  pages = {126--130},
  %abstract = {Deep learning has been widely used recently for sound event detection and classification. Its success is linked to the availability of sufficiently large datasets, possibly with corresponding annotations when supervised learning is considered. In bioacoustic applications, most tasks come with few labelled training data, because annotating long recordings is time consuming and costly. Therefore supervised learning is not the best suited approach to solve bioacoustic tasks. The bioacoustic community recasted the problem of sound event detection within the framework of few-shot learning, i.e. training a system with only few labeled examples. The few-shot bioacoustic sound event detection task in the DCASE challenge focuses on detecting events in long audio recordings given only five annotated examples for each class of interest. In this paper, we show that learning a rich feature extractor from scratch can be achieved by leveraging data augmentation using a supervised contrastive learning framework. We highlight the ability of this framework to transfer well for five-shot event detection on previously unseen classes in the training data. We obtain an F-score of 63.46% on the validation set and 42.7% on the test set, ranking second in the DCASE challenge. We provide an ablation study for the critical choices of data augmentation techniques as well as for the learning strategy applied on the training set. Our code is available on Github.},
  %eprint={2309.00878},
  %archivePrefix={arXiv},
  %primaryClass={cs.SD},
}

% cow stuff
@inproceedings{Vidana-Vila2023,
  author = {Vidaña-Vila, Ester and Malé, Jordi and Freixes, Marc and Solís-Cifré, Mireia and Jiménez, Miquel and Larrondo, Cristian and Guevara, Raúl and Miranda, Joana and Duboc, Leticia and Mainau, Eva and Llonch, Pol and Alsina-Pagès, Rosa Ma},
  title = {Automatic Detection of Cow Vocalizations Using Convolutional Neural Networks},
  booktitle = {Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)},
  month = {9},
  year = {2023},
  pages = {206--210},
  %address = {Tampere, Finland},
  %abstract = {The well-being of animals holds significant importance in our society. Apart from the ethical concerns, recent studies have highlighted the correlation of animal growth, reproductive potential, and overall productivity with animal welfare. In this context, the vocalizations of cows have emerged as a valuable indicator of their well-being for veterinary researchers, but gathering and labelling the vocalizations for their in-depth study is time-consuming and labour-intensive. For this reason, in this work, we present an acoustic event detection algorithm that has been trained and validated with different setups using acoustic data collected from two different farms. The experimental set-up consists of a Convolutional Neural Network followed by a post-processing stage for the detection of vocalizations, so veterinary researchers can easily analyze them. The experimental evaluation assesses the importance of selecting the convenient post-processing and overlapping acoustic window for finding new vocalizations. Furthermore, the study evaluates the significance of using data collected specifically from the same farm for acoustic event detection, as opposed to employing data from a different farm. Results show that by merging training data from different farms, including the farm that is being evaluated, an F1 score of 57.40\% and a recall of 74.05\% can be achieved.},
}

% fewshot learning, dcase
@article{Nolasco2023,
  author = {Ines Nolasco and Shubhr Singh and Veronica Morfi and Vincent Lostanlen and Ariana Strandburg-Peshkin and Ester Vidaña-Vila and Lisa Gill and Hanna Pamuła and Helen Whitehead and Ivan Kiskin and Frants H. Jensen and Joe Morford and Michael G. Emmerson and Elisabetta Versace and Emily Grout and Haohe Liu and Burooj Ghani and Dan Stowell},
  title = {Learning to detect an animal sound from five examples},
  journal = {Ecological Informatics},
  volume = {77},
  pages = {102258},
  year = {2023},
  issn = {1574-9541},
  %doi = {https://doi.org/10.1016/j.ecoinf.2023.102258},
  %url = {https://www.sciencedirect.com/science/article/pii/S157495412300287X},
  %keywords = {Bioacoustics, Deep learning, Event detection, Few-shot learning},
  %abstract = {Automatic detection and classification of animal sounds has many applications in biodiversity monitoring and animal behavior. In the past twenty years, the volume of digitised wildlife sound available has massively increased, and automatic classification through deep learning now shows strong results. However, bioacoustics is not a single task but a vast range of small-scale tasks (such as individual ID, call type, emotional indication) with wide variety in data characteristics, and most bioacoustic tasks do not come with strongly-labelled training data. The standard paradigm of supervised learning, focussed on a single large-scale dataset and/or a generic pre-trained algorithm, is insufficient. In this work we recast bioacoustic sound event detection within the AI framework of few-shot learning. We adapt this framework to sound event detection, such that a system can be given the annotated start/end times of as few as 5 events, and can then detect events in long-duration audio—even when the sound category was not known at the time of algorithm training. We introduce a collection of open datasets designed to strongly test a system's ability to perform few-shot sound event detections, and we present the results of a public contest to address the task. Our analysis shows that prototypical networks are a very common used strategy and they perform well when enhanced with adaptations for general characteristics of animal sounds. However, systems with high time resolution capabilities perform the best in this challenge. We demonstrate that widely-varying sound event durations are an important factor in performance, as well as non-stationarity, i.e. gradual changes in conditions throughout the duration of a recording. For fine-grained bioacoustic recognition tasks without massive annotated training data, our analysis demonstrate that few-shot sound event detection is a powerful new method, strongly outperforming traditional signal-processing detection methods in the fully automated scenario.}
}

% conservation
@article{Teixeira2019Bmoavbfc,
  author = {Daniella Teixeira and Martine Maron and Berndt J. van Rensburg},
  title = {Bioacoustic monitoring of animal vocal behavior for conservation},
  issue = {8},
  journal = {Conservation Science and Practice},
  volume = {1},
  year = {2019},
  %doi = {10.1111/csp2.72},
  %issn = {25784854},
  %abstract = {The popularity of bioacoustics for threatened species monitoring has surged. Large volumes of acoustic data can be collected autonomously and remotely with minimal human effort. The approach is commonly used to detect cryptic species and, more recently, to estimate abundance or density. However, the potential for conservation-relevant information to be derived from acoustic signatures associated with particular behavior is less well-exploited. Animal vocal behavior can reveal important information about critical life history events. In this study, we argue that the overlap of the disciplines of bioacoustics, vocal communication, and conservation behavior—thus, “acoustic conservation behavior”—has much to offer threatened species monitoring. In particular, vocalizations can serve as indicators of behavioral states and contexts that provide insight into populations as it relates to their conservation. We explore the information available from monitoring species' vocalizations that relate to reproduction and recruitment, alarm and defense, and social behavior, and how this information could translate into potential conservation benefits. While there are still challenges to processing acoustic data, we conclude that acoustic conservation behavior may improve threatened species monitoring where vocalizations reveal behaviors that are informative for management and decision-making.},
}

% cows, acoustic sensors, automated detection
@article{Shorten2023Asfad,
  author = {P.R. Shorten and L.B. Hunter},
  title = {Acoustic sensors for automated detection of cow vocalization duration and type},
  journal = {Computers and Electronics in Agriculture},
  volume = {208},
  pages = {107760},
  year = {2023},
  %issn = {0168-1699},
  %doi = {https://doi.org/10.1016/j.compag.2023.107760},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168169923001485},
  %keywords = {Cow, Acoustic, Machine learning, Neural network, Vocalization, Livestock},
  %abstract = {Acoustic technologies provide a non-invasive method to generate information about cow vocalization. This study demonstrated that collar attached acoustic sensors can differentiate between cow vocalization and background sounds obtained from cows under grazing conditions. The overall accuracy of the vocalization classification model was 99.5% in the test dataset, based on a total of 709 vocalization recordings from 10 cows. Vocalization samples were obtained from 10 trial cows, with a frequency that ranged from 3 to 452 vocalizations per cow. Algorithms were also developed to differentiate between three different cow vocalization classes (Open mouth, Closed mouth, Mixed mouth (Closed mouth followed by Open mouth)), with a model accuracy of 85% in the test dataset. Most cows demonstrated all three types of vocalization, and the between-cow variability in the probability of mixed vocalization per vocalization had a standard deviation of 0.13 relative an average probability of 0.74. The duration of the 709 individual vocalizations ranged from 0.88 to 3.37 s, with an average of 1.76 s and a standard deviation of 0.36 s. There was a between-cow variation in the duration of vocalization, with a standard deviation of 0.12 ± 0.04 s (P < 0.01). The performance of the model for the duration of vocalization had a coefficient of determination of R2 = 0.84 in the test dataset. Models to predict the proportion of a vocalization that is a closed vocalization had a coefficient of determination of R2 = 0.72 in the test dataset. The proportion of a mixed vocalization that is a closed vocalization ranged from 0.04 to 0.92 and had an average of 0.38 and a standard deviation of 0.15. There was also between-cow variation in the proportion of a mixed vocalization that is a closed vocalization, with a standard deviation of 0.08 ± 0.023 (P < 0.01). The mixed vocalization had an acoustic spectral and temporal pattern that was unique to the cow that generated the vocalization, and classification models for voice recognition had an accuracy of 80% in the test dataset. A prototype spectral unmixing algorithm was also developed to use the ensemble of cow-collar acoustic recordings from each cow to assign each cow vocalization to the cow that generated the vocalization. This study demonstrated that there is significant between-cow variability in cow vocalization traits, and that these traits can be determined using cow-attached acoustic sensors to provide information on the welfare and state of the animal.},
}

% animal welfare, vocalization
@article{Manteuffel2004Vofa,
  author = {Gerhard Manteuffel and Birger Puppe and Peter C Sch{\"o}n},
  title = {Vocalization of farm animals as a measure of welfare},
  journal = {Applied Animal Behaviour Science},
  volume = {88},
  number = {1},
  pages = {163-182},
  year = {2004},
  %issn = {0168-1591},
  %doi = {https://doi.org/10.1016/j.applanim.2004.02.012},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168159104000565},
  keywords = {Vocalization, Farm animals, Pig, Cattle, Poultry, Animal welfare},
  abstract = {Emotionally relevant external events, hormone concentrations affecting mood and appetitive behaviour, thirst and hunger are able to stimulate a complex central nervous network that regulates endocrine feedback and behaviour in order to maintain or regain homeostasis. Particular states of mood or emotion may thus be accompanied by specific behaviours, vocalization being one of them. Hence, in farm animals vocalizations may supply us with hints on their well-being in an easy way, given that the meanings of the respective calls are well-established. Then, it is possible to judge acoustically uttered current needs and impaired welfare by non-invasive, continuous monitoring. Vocalizations may also modulate emotions of the receivers such that welfare may also be affected in conspecifics hearing distress utterances, e.g., in an abattoir. For these reasons, the analysis of farm animal vocalization has gained increasing interest in the last years and a variety of attempts to decode the meaning has been made. Concentrating on important farm animal species (pig, cattle, poultry) an overview of the present state-of-the-art in this discipline is given and present problems as well as possible future developments are discussed. Modern techniques of sound analysis have provided tools to discriminate, analyse and classify specific vocalizations. Taking advantage of this, future bioacoustical research for welfare assessment should focus on comprehensive studies of a broad spectrum of species specific distress vocalizations. Increasingly precise attributions of such utterances to environments, behavioural contexts and relevant physiological parameters will lead to a deeper understanding of their meaning and significance with respect to well-being of farm animals. The result will offer applicable acoustic tools for farming environments where non-invasive techniques for welfare judgements are urgently needed.}
}

% welfare survey
@misc{Mcloughlin2019,
  author = {Michael P. Mcloughlin and Rebecca Stewart and Alan G. McElligott},
  title = {Automated bioacoustics: Methods in ecology and conservation and their potential for animal welfare monitoring},
  issue = {155},
  journal = {Journal of the Royal Society Interface},
  volume = {16},
  year = {2019},
  %abstract = {Vocalizations carry emotional, physiological and individual information. This suggests that they may serve as potentially useful indicators for inferring animal welfare. At the same time, automated methods for analysing and classifying sound have developed rapidly, particularly in the fields of ecology, conservation and sound scene classification. These methods are already used to automatically classify animal vocalizations, for example, in identifying animal species and estimating numbers of individuals. Despite this potential, they have not yet found widespread application in animal welfare monitoring. In this review, we first discuss current trends in sound analysis for ecology, conservation and sound classification. Following this, we detail the vocalizations produced by three of the most important farm livestock species: chickens (Gallus gallus domesticus), pigs (Sus scrofa domesticus) and cattle (Bos taurus). Finally, we describe how these methods can be applied to monitor animal welfare with new potential for developing automated methods for large-scale farming.},
  %doi = {10.1098/rsif.2019.0225},
  %issn = {17425662},
}

% soundscape ecology
@article{Pijanowski2011SoundscapeEcology,
  author = {Bryan C. Pijanowski and Luis J. Villanueva-Rivera and Sarah L. Dumyahn and Almo Farina and Bernie L. Krause and Brian M. Napoletano and Stuart H. Gage and Nadia Pieretti},
  title = {Soundscape ecology: The science of sound in the landscape},
  issue = {3},
  journal = {BioScience},
  volume = {61},
  year = {2011},
  %abstract = {This article presents a unifying theory of soundscape ecology, which brings the idea of the soundscape the collection of sounds that emanate from landscapesinto a research and application focus. Our conceptual framework of soundscape ecology is based on the causes and consequences of biological (biophony), geophysical (geophony), and human-produced (anthrophony) sounds. We argue that soundscape ecology shares many parallels with landscape ecology, and it should therefore be considered a branch of this maturing field. We propose a research agenda for soundscape ecology that includes six areas: (1) measurement and analytical challenges, (2) spatial-temporal dynamics, (3) soundscape linkage to environmental covariates, (4) human impacts on the soundscape, (5) soundscape impacts on humans, and (6) soundscape impacts on ecosystems. We present case studies that illustrate different approaches to understanding soundscape dynamics. Because soundscapes are our auditory link to nature, we also argue for their protection, using the knowledge of how sounds are produced by the environment and humans. © 2011 by American Institute of Biological Sciences. All rights reserved.},
  %doi = {10.1525/bio.2011.61.3.6},
  %issn = {00063568},
}

% animal call
@article{Towsey2012,
  author = {Michael Towsey and Birgit Planitz and Alfredo Nantes and Jason Wimmer and Paul Roe},
  title = {A toolbox for animal call recognition},
  issue = {2},
  journal = {Bioacoustics},
  volume = {21},
  year = {2012},
  %doi = {10.1080/09524622.2011.648753},
  %issn = {09524622},
  %abstract = {Monitoring the natural environment is increasingly important as habit degradation and climate change reduce the world's biodiversity. We have developed software tools and applications to assist ecologists with the collection and analysis of acoustic data at large spatial and temporal scales. One of our key objectives is automated animal call recognition, and our approach has three novel attributes. First, we work with raw environmental audio, contaminated by noise and artefacts and containing calls that vary greatly in volume depending on the animal's proximity to the microphone. Second, initial experimentation suggested that no single recognizer could deal with the enormous variety of calls. Therefore, we developed a toolbox of generic recognizers to extract invariant features for each call type. Third, many species are cryptic and offer little data with which to train a recognizer. Many popular machine learning methods require large volumes of training and validation data and considerable time and expertise to prepare. Consequently we adopt bootstrap techniques that can be initiated with little data and refined subsequently. In this paper, we describe our recognition tools and present results for real ecological problems. © 2012 Taylor & Francis.},
}


% bird detection, survey
@inproceedings{Stowell2016Bdia,
  author = {Dan Stowell and Mike Wood and Yannis Stylianou and Herve Glotin},
  title = {Bird detection in audio: A survey and a challenge},
  booktitle = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
  volume = {2016-November},
  year = {2016},
  %abstract = {Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection.},
  %doi = {10.1109/MLSP.2016.7738875},
  %issn = {21610371},
}


% density estimation of animals
@article{Marques2013Eapdupa,
  author = {Tiago A. Marques and Len Thomas and Stephen W. Martin and David K. Mellinger and Jessica A. Ward and David J. Moretti and Danielle Harris and Peter L. Tyack},
  title = {Estimating animal population density using passive acoustics},
  issue = {2},
  journal = {Biological Reviews},
  volume = {88},
  year = {2013},
  %doi = {10.1111/brv.12001},
  %issn = {14647931},
  %abstract = {Reliable estimation of the size or density of wild animal populations is very important for effective wildlife management, conservation and ecology. Currently, the most widely used methods for obtaining such estimates involve either sighting animals from transect lines or some form of capture-recapture on marked or uniquely identifiable individuals. However, many species are difficult to sight, and cannot be easily marked or recaptured. Some of these species produce readily identifiable sounds, providing an opportunity to use passive acoustic data to estimate animal density. In addition, even for species for which other visually based methods are feasible, passive acoustic methods offer the potential for greater detection ranges in some environments (e.g. underwater or in dense forest), and hence potentially better precision. Automated data collection means that surveys can take place at times and in places where it would be too expensive or dangerous to send human observers. Here, we present an overview of animal density estimation using passive acoustic data, a relatively new and fast-developing field. We review the types of data and methodological approaches currently available to researchers and we provide a framework for acoustics-based density estimation, illustrated with examples from real-world case studies. We mention moving sensor platforms (e.g. towed acoustics), but then focus on methods involving sensors at fixed locations, particularly hydrophones to survey marine mammals, as acoustic-based density estimation research to date has been concentrated in this area. Primary among these are methods based on distance sampling and spatially explicit capture-recapture. The methods are also applicable to other aquatic and terrestrial sound-producing taxa. We conclude that, despite being in its infancy, density estimation based on passive acoustic data likely will become an important method for surveying a number of diverse taxa, such as sea mammals, fish, birds, amphibians, and insects, especially in situations where inferences are required over long periods of time. There is considerable work ahead, with several potentially fruitful research areas, including the development of (i) hardware and software for data acquisition, (ii) efficient, calibrated, automated detection and classification systems, and (iii) statistical approaches optimized for this application. Further, survey design will need to be developed, and research is needed on the acoustic behaviour of target species. Fundamental research on vocalization rates and group sizes, and the relation between these and other factors such as season or behaviour state, is critical. Evaluation of the methods under known density scenarios will be important for empirically validating the approaches presented here. © 2012 Cambridge Philosophical Society.},
}

% soundscape
@article{Sethi2022Spsoitf,
  author = {Sarab S. Sethi and Robert M. Ewers and Nick S. Jones and Jani Sleutel and Adi Shabrani and Nursyamin Zulkifli and Lorenzo Picinali},
  title = {Soundscapes predict species occurrence in tropical forests},
  journal = {Oikos},
  volume = {2022},
  year = {2022},
  issue = {3},
  %doi = {10.1111/oik.08525},
  %issn = {16000706},
  %abstract = {Accurate occurrence data is necessary for the conservation of keystone or endangered species, but acquiring it is usually slow, laborious and costly. Automated acoustic monitoring offers a scalable alternative to manual surveys but identifying species vocalisations requires large manually annotated training datasets, and is not always possible (e.g. for lesser studied or silent species). A new approach is needed that rapidly predicts species occurrence using smaller and more coarsely labelled audio datasets. We investigated whether local soundscapes could be used to infer the presence of 32 avifaunal and seven herpetofaunal species in 20 min recordings across a tropical forest degradation gradient in Sabah, Malaysia. Using acoustic features derived from a convolutional neural network (CNN), we characterised species indicative soundscapes by training our models on a temporally coarse labelled point-count dataset. Soundscapes successfully predicted the occurrence of 34 out of the 39 species across the two taxonomic groups, with area under the curve (AUC) metrics from 0.53 up to 0.87. The highest accuracies were achieved for species with strong temporal occurrence patterns. Soundscapes were a better predictor of species occurrence than above-ground carbon density – a metric often used to quantify habitat quality across forest degradation gradients. Our results demonstrate that soundscapes can be used to efficiently predict the occurrence of a wide variety of species and provide a new direction for data driven large-scale assessments of habitat suitability.},
}

% monitoring network,
@article{Sethi2020SA,
  author = {Sethi, Sarab S. and Ewers, Robert M. and Jones, Nick S. and Signorelli, Aaron and Picinali, Lorenzo and Orme, Christopher David L.},
  title = {SAFE Acoustics: An open-source, real-time eco-acoustic monitoring network in the tropical rainforests of Borneo},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {10},
  pages = {1182-1185},
  year = {2020},
  %keywords = {applied ecology, bioinformatics, community ecology, conservation, monitoring (community ecology), monitoring (population ecology), software, surveys},
  %doi = {https://doi.org/10.1111/2041-210X.13438},
  %url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13438},
  %eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13438},
  %abstract = {Abstract Automated monitoring approaches offer an avenue to unlocking large-scale insight into how ecosystems respond to human pressures. However, since data collection and data analyses are often treated independently, there are currently no open-source examples of end-to-end, real-time ecological monitoring networks. Here, we present the complete implementation of an autonomous acoustic monitoring network deployed in the tropical rainforests of Borneo. Real-time audio is uploaded remotely from the field, indexed by a central database, and delivered via an API to a public-facing website. We provide the open-source code and design of our monitoring devices, the central web2py database, and the ReactJS website. Furthermore, we demonstrate an extension of this infrastructure to deliver real-time analyses of the eco-acoustic data. By detailing a fully functional, open source, and extensively tested design, our work will accelerate the rate at which fully autonomous monitoring networks mature from technological curiosities, and towards genuinely impactful tools in ecology.},
}

% schlüter, bird
@inproceedings{Schlter2018BirdIF,
  author={Jan Schl{\"u}ter},
  title={Bird Identification from Timestamped, Geotagged Audio Recordings},
  booktitle={Conference and Labs of the Evaluation Forum},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:51941747},
}

% arbimon
@article{Aide2013Arbimon,
  author = {T. Mitchell Aide and Carlos Corrada-Bravo and Marconi Campos-Cerqueira and Carlos Milan and Giovany Vega and Rafae Alvarez},
  title = {Real-time bioacoustics monitoring and automated species identification},
  abstract = {Traditionally, animal species diversity and abundance is assessed using a variety of methods that are generally costly, limited in space and time, and most importantly, they rarely include a permanent record. Given the urgency of climate change and the loss of habitat, it is vital that we use new technologies to improve and expand global biodiversity monitoring to thousands of sites around the world. In this article, we describe the acoustical component of the Automated Remote BiodiversityMonitoring Network (ARBIMON), a novel combination of hardware and software for automating data acquisition, data management, and species identification based on audio recordings. The major components of the cyberinfrastructure include: a solar powered remote monitoring station that sends 1-min recordings every 10 min to a base station, which relays the recordings in real-time to the project server, where the recordings are processed and uploaded to the project website (arbimon.net). Along with a module for viewing, listening, and annotating recordings, the website includes a species identification interface to help users create machine learning algorithms to automate species identification. To demonstrate the system we present data on the vocal activity patterns of birds, frogs, insects, and mammals from Puerto Rico and Costa Rica. © 2013 Aide et al.},
  doi = {10.7717/peerj.103},
  issn = {21678359},
  issue = {1},
  journal = {PeerJ},
  volume = {2013},
  year = {2013},
}

% bioacoustic ai
@online{BioacousticAi,
  author = {{Naturalis Biodiversity Center}},
  title = {BioAcoustic AI},
  year = 2023,
  url = {https://bioacousticai.eu/},
  urldate = {2024-01-30}
}

% dcase
@book{DCASE2023Workshop,
  author = {Fuentes, Magdalena and Heittola, Toni and Imoto, Keisuke and Mesaros, Annamaria and Politis, Archontis and Serizel, Romain and Virtanen, Tuomas},
  title = {Proceedings of the 8th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2023)},
  year = {2023},
  publisher = {Tampere University},
  isbn = {978-952-03-3171-9},
  month = {9},
  address = {Tampere, Finland},
}

% cow stuff
@inproceedings{Vidana-Vila2023,
  author = {Vidaña-Vila, Ester and Malé, Jordi and Freixes, Marc and Solís-Cifré, Mireia and Jiménez, Miquel and Larrondo, Cristian and Guevara, Raúl and Miranda, Joana and Duboc, Leticia and Mainau, Eva and Llonch, Pol and Alsina-Pagès, Rosa Ma},
  title = {Automatic Detection of Cow Vocalizations Using Convolutional Neural Networks},
  booktitle = {Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)},
  address = {Tampere, Finland},
  month = {9},
  year = {2023},
  pages = {206--210},
  abstract = {The well-being of animals holds significant importance in our society. Apart from the ethical concerns, recent studies have highlighted the correlation of animal growth, reproductive potential, and overall productivity with animal welfare. In this context, the vocalizations of cows have emerged as a valuable indicator of their well-being for veterinary researchers, but gathering and labelling the vocalizations for their in-depth study is time-consuming and labour-intensive. For this reason, in this work, we present an acoustic event detection algorithm that has been trained and validated with different setups using acoustic data collected from two different farms. The experimental set-up consists of a Convolutional Neural Network followed by a post-processing stage for the detection of vocalizations, so veterinary researchers can easily analyze them. The experimental evaluation assesses the importance of selecting the convenient post-processing and overlapping acoustic window for finding new vocalizations. Furthermore, the study evaluates the significance of using data collected specifically from the same farm for acoustic event detection, as opposed to employing data from a different farm. Results show that by merging training data from different farms, including the farm that is being evaluated, an F1 score of 57.40\% and a recall of 74.05\% can be achieved.},
}

% bioacoustic roadmap
@article{Stowell2022,
  author       = {Dan Stowell},
  title        = {Computational bioacoustics with deep learning: a review and roadmap},
  journal      = {CoRR},
  volume       = {abs/2112.06725},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.06725},
  eprinttype    = {arXiv},
  eprint       = {2112.06725},
  timestamp    = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-06725.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
}

% birdnet
@article{Kahl2021,
  author = {Kahl, Stefan and Wood, Connor M. and Eibl, Maximilian and Klinck, Holger},
  title = {{BirdNET: A deep learning solution for avian diversity monitoring}},
  abstract = {Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286 h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670 h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4 years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation.},
  doi = {10.1016/j.ecoinf.2021.101236},
  issn = {15749541},
  journal = {Ecological Informatics},
  volume = {61},
  year = {2021},
}

% bioacoustic monitoring
@article{DeCamargo2019,
  author = {de Camargo, Ulisses and Roslin, Tomas and Ovaskainen, Otso},
  title = {{Spatio-temporal scaling of biodiversity in acoustic tropical bird communities}},
  abstract = {Automated analysis of acoustic communities is a rapidly emerging approach for the characterization and monitoring of biodiversity. To evaluate its utility, we should verify that such ‘bioacoustics' can accurately detect ecological signal in spatiotemporal acoustic data. Targeting the ‘Biological Dynamics of Forest Fragments Project' sites in Brazil, we ask: What is the relative contribution of the spatial, temporal and habitat dimension to variation in bird acoustic communities in a previously fragmented tropical rainforest? Does the functional diversity of bird communities scale similarly to space and time as does species diversity, when both are recorded by bioacoustics means? Overall, is the imprint of landscape fragmentation 30 years ago still audible in the present-day soundscape? We sampled forty-four sites in secondary forest and 107 sites in old-growth forest, resulting in 11 000 h of audio recordings. We detected 60 bird species with satisfactory precision and recovered a linear log–log relation between sampling time and species diversity. Sites in primary forest host more species than sites in secondary forest, but the difference decreased with sampling time, as the slope was slightly higher in secondary than primary forests. Functional diversity, as exposed by vocalizing birds, accumulates faster than does species diversity. The similarity among local communities decreases with distance in both time and space, but stability in time is remarkably high: two acoustic samples from the same site one year (or more) apart prove more similar than two samples taken at the same time but from sites situated just a few hundred meters apart. These findings suggest that habitat modification can be heard as a long-lasting imprint on the soundscape of regenerating habitats and identify soundscape–area and soundscape–time relations as a promising tool for biodiversity research, applied biomonitoring and restoration ecology.},
  doi = {10.1111/ecog.04544},
  issn = {16000587},
  journal = {Ecography},
  number = {11},
  volume = {42},
  year = {2019},
}