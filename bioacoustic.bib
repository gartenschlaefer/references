# --
# bioacoustic reference collections

% animal call
@article{Towsey2012,
   author = {Michael Towsey and Birgit Planitz and Alfredo Nantes and Jason Wimmer and Paul Roe},
   title = {A toolbox for animal call recognition},
   abstract = {Monitoring the natural environment is increasingly important as habit degradation and climate change reduce the world's biodiversity. We have developed software tools and applications to assist ecologists with the collection and analysis of acoustic data at large spatial and temporal scales. One of our key objectives is automated animal call recognition, and our approach has three novel attributes. First, we work with raw environmental audio, contaminated by noise and artefacts and containing calls that vary greatly in volume depending on the animal's proximity to the microphone. Second, initial experimentation suggested that no single recognizer could deal with the enormous variety of calls. Therefore, we developed a toolbox of generic recognizers to extract invariant features for each call type. Third, many species are cryptic and offer little data with which to train a recognizer. Many popular machine learning methods require large volumes of training and validation data and considerable time and expertise to prepare. Consequently we adopt bootstrap techniques that can be initiated with little data and refined subsequently. In this paper, we describe our recognition tools and present results for real ecological problems. © 2012 Taylor & Francis.},
   doi = {10.1080/09524622.2011.648753},
   issn = {09524622},
   issue = {2},
   journal = {Bioacoustics},
   volume = {21},
   year = {2012},
}


% bird detection, survey
@inproceedings{Stowell2016,
  author = {Dan Stowell and Mike Wood and Yannis Stylianou and Herve Glotin},
  title = {Bird detection in audio: A survey and a challenge},
  abstract = {Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection.},
  doi = {10.1109/MLSP.2016.7738875},
  issn = {21610371},
  journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
  volume = {2016-November},
  year = {2016},
}


% density estimation of animals
@article{Marques2013Eapdupa,
  author = {Tiago A. Marques and Len Thomas and Stephen W. Martin and David K. Mellinger and Jessica A. Ward and David J. Moretti and Danielle Harris and Peter L. Tyack},
  title = {Estimating animal population density using passive acoustics},
  abstract = {Reliable estimation of the size or density of wild animal populations is very important for effective wildlife management, conservation and ecology. Currently, the most widely used methods for obtaining such estimates involve either sighting animals from transect lines or some form of capture-recapture on marked or uniquely identifiable individuals. However, many species are difficult to sight, and cannot be easily marked or recaptured. Some of these species produce readily identifiable sounds, providing an opportunity to use passive acoustic data to estimate animal density. In addition, even for species for which other visually based methods are feasible, passive acoustic methods offer the potential for greater detection ranges in some environments (e.g. underwater or in dense forest), and hence potentially better precision. Automated data collection means that surveys can take place at times and in places where it would be too expensive or dangerous to send human observers. Here, we present an overview of animal density estimation using passive acoustic data, a relatively new and fast-developing field. We review the types of data and methodological approaches currently available to researchers and we provide a framework for acoustics-based density estimation, illustrated with examples from real-world case studies. We mention moving sensor platforms (e.g. towed acoustics), but then focus on methods involving sensors at fixed locations, particularly hydrophones to survey marine mammals, as acoustic-based density estimation research to date has been concentrated in this area. Primary among these are methods based on distance sampling and spatially explicit capture-recapture. The methods are also applicable to other aquatic and terrestrial sound-producing taxa. We conclude that, despite being in its infancy, density estimation based on passive acoustic data likely will become an important method for surveying a number of diverse taxa, such as sea mammals, fish, birds, amphibians, and insects, especially in situations where inferences are required over long periods of time. There is considerable work ahead, with several potentially fruitful research areas, including the development of (i) hardware and software for data acquisition, (ii) efficient, calibrated, automated detection and classification systems, and (iii) statistical approaches optimized for this application. Further, survey design will need to be developed, and research is needed on the acoustic behaviour of target species. Fundamental research on vocalization rates and group sizes, and the relation between these and other factors such as season or behaviour state, is critical. Evaluation of the methods under known density scenarios will be important for empirically validating the approaches presented here. © 2012 Cambridge Philosophical Society.},
  doi = {10.1111/brv.12001},
  issn = {14647931},
  issue = {2},
  journal = {Biological Reviews},
  volume = {88},
  year = {2013},
}

% soundscape
@article{Sethi2022Spsoitf,
  author = {Sarab S. Sethi and Robert M. Ewers and Nick S. Jones and Jani Sleutel and Adi Shabrani and Nursyamin Zulkifli and Lorenzo Picinali},
  title = {Soundscapes predict species occurrence in tropical forests},
  abstract = {Accurate occurrence data is necessary for the conservation of keystone or endangered species, but acquiring it is usually slow, laborious and costly. Automated acoustic monitoring offers a scalable alternative to manual surveys but identifying species vocalisations requires large manually annotated training datasets, and is not always possible (e.g. for lesser studied or silent species). A new approach is needed that rapidly predicts species occurrence using smaller and more coarsely labelled audio datasets. We investigated whether local soundscapes could be used to infer the presence of 32 avifaunal and seven herpetofaunal species in 20 min recordings across a tropical forest degradation gradient in Sabah, Malaysia. Using acoustic features derived from a convolutional neural network (CNN), we characterised species indicative soundscapes by training our models on a temporally coarse labelled point-count dataset. Soundscapes successfully predicted the occurrence of 34 out of the 39 species across the two taxonomic groups, with area under the curve (AUC) metrics from 0.53 up to 0.87. The highest accuracies were achieved for species with strong temporal occurrence patterns. Soundscapes were a better predictor of species occurrence than above-ground carbon density – a metric often used to quantify habitat quality across forest degradation gradients. Our results demonstrate that soundscapes can be used to efficiently predict the occurrence of a wide variety of species and provide a new direction for data driven large-scale assessments of habitat suitability.},
  doi = {10.1111/oik.08525},
  issn = {16000706},
  issue = {3},
  journal = {Oikos},
  volume = {2022},
  year = {2022},
}

% schlüter, bird
@inproceedings{Schlter2018BirdIF,
  author={Jan Schl{\"u}ter},
  title={Bird Identification from Timestamped, Geotagged Audio Recordings},
  booktitle={Conference and Labs of the Evaluation Forum},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:51941747},
}

% arbimon
@article{Aide2013Arbimon,
  author = {T. Mitchell Aide and Carlos Corrada-Bravo and Marconi Campos-Cerqueira and Carlos Milan and Giovany Vega and Rafae Alvarez},
  title = {Real-time bioacoustics monitoring and automated species identification},
  abstract = {Traditionally, animal species diversity and abundance is assessed using a variety of methods that are generally costly, limited in space and time, and most importantly, they rarely include a permanent record. Given the urgency of climate change and the loss of habitat, it is vital that we use new technologies to improve and expand global biodiversity monitoring to thousands of sites around the world. In this article, we describe the acoustical component of the Automated Remote BiodiversityMonitoring Network (ARBIMON), a novel combination of hardware and software for automating data acquisition, data management, and species identification based on audio recordings. The major components of the cyberinfrastructure include: a solar powered remote monitoring station that sends 1-min recordings every 10 min to a base station, which relays the recordings in real-time to the project server, where the recordings are processed and uploaded to the project website (arbimon.net). Along with a module for viewing, listening, and annotating recordings, the website includes a species identification interface to help users create machine learning algorithms to automate species identification. To demonstrate the system we present data on the vocal activity patterns of birds, frogs, insects, and mammals from Puerto Rico and Costa Rica. © 2013 Aide et al.},
  doi = {10.7717/peerj.103},
  issn = {21678359},
  issue = {1},
  journal = {PeerJ},
  volume = {2013},
  year = {2013},
}

% bioacoustic ai
@online{BioacousticAi,
  author = {{Naturalis Biodiversity Center}},
  title = {BioAcoustic AI},
  year = 2023,
  url = {https://bioacousticai.eu/},
  urldate = {2024-01-30}
}

% dcase
@book{DCASE2023Workshop,
  author = {Fuentes, Magdalena and Heittola, Toni and Imoto, Keisuke and Mesaros, Annamaria and Politis, Archontis and Serizel, Romain and Virtanen, Tuomas},
  title = {Proceedings of the 8th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2023)},
  year = {2023},
  publisher = {Tampere University},
  isbn = {978-952-03-3171-9},
  month = {9},
  address = {Tampere, Finland},
}

% cow stuff
@inproceedings{Vidana-Vila2023,
  author = {Vidaña-Vila, Ester and Malé, Jordi and Freixes, Marc and Solís-Cifré, Mireia and Jiménez, Miquel and Larrondo, Cristian and Guevara, Raúl and Miranda, Joana and Duboc, Leticia and Mainau, Eva and Llonch, Pol and Alsina-Pagès, Rosa Ma},
  title = {Automatic Detection of Cow Vocalizations Using Convolutional Neural Networks},
  booktitle = {Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)},
  address = {Tampere, Finland},
  month = {9},
  year = {2023},
  pages = {206--210},
  abstract = {The well-being of animals holds significant importance in our society. Apart from the ethical concerns, recent studies have highlighted the correlation of animal growth, reproductive potential, and overall productivity with animal welfare. In this context, the vocalizations of cows have emerged as a valuable indicator of their well-being for veterinary researchers, but gathering and labelling the vocalizations for their in-depth study is time-consuming and labour-intensive. For this reason, in this work, we present an acoustic event detection algorithm that has been trained and validated with different setups using acoustic data collected from two different farms. The experimental set-up consists of a Convolutional Neural Network followed by a post-processing stage for the detection of vocalizations, so veterinary researchers can easily analyze them. The experimental evaluation assesses the importance of selecting the convenient post-processing and overlapping acoustic window for finding new vocalizations. Furthermore, the study evaluates the significance of using data collected specifically from the same farm for acoustic event detection, as opposed to employing data from a different farm. Results show that by merging training data from different farms, including the farm that is being evaluated, an F1 score of 57.40\% and a recall of 74.05\% can be achieved.},
}

% bioacoustic roadmap
@article{Stowell2022,
  author       = {Dan Stowell},
  title        = {Computational bioacoustics with deep learning: a review and roadmap},
  journal      = {CoRR},
  volume       = {abs/2112.06725},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.06725},
  eprinttype    = {arXiv},
  eprint       = {2112.06725},
  timestamp    = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-06725.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
}

% birdnet
@article{Kahl2021,
  author = {Kahl, Stefan and Wood, Connor M. and Eibl, Maximilian and Klinck, Holger},
  title = {{BirdNET: A deep learning solution for avian diversity monitoring}},
  abstract = {Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286 h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670 h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4 years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation.},
  doi = {10.1016/j.ecoinf.2021.101236},
  issn = {15749541},
  journal = {Ecological Informatics},
  volume = {61},
  year = {2021},
}

% bioacoustic monitoring
@article{DeCamargo2019,
  author = {de Camargo, Ulisses and Roslin, Tomas and Ovaskainen, Otso},
  title = {{Spatio-temporal scaling of biodiversity in acoustic tropical bird communities}},
  abstract = {Automated analysis of acoustic communities is a rapidly emerging approach for the characterization and monitoring of biodiversity. To evaluate its utility, we should verify that such ‘bioacoustics' can accurately detect ecological signal in spatiotemporal acoustic data. Targeting the ‘Biological Dynamics of Forest Fragments Project' sites in Brazil, we ask: What is the relative contribution of the spatial, temporal and habitat dimension to variation in bird acoustic communities in a previously fragmented tropical rainforest? Does the functional diversity of bird communities scale similarly to space and time as does species diversity, when both are recorded by bioacoustics means? Overall, is the imprint of landscape fragmentation 30 years ago still audible in the present-day soundscape? We sampled forty-four sites in secondary forest and 107 sites in old-growth forest, resulting in 11 000 h of audio recordings. We detected 60 bird species with satisfactory precision and recovered a linear log–log relation between sampling time and species diversity. Sites in primary forest host more species than sites in secondary forest, but the difference decreased with sampling time, as the slope was slightly higher in secondary than primary forests. Functional diversity, as exposed by vocalizing birds, accumulates faster than does species diversity. The similarity among local communities decreases with distance in both time and space, but stability in time is remarkably high: two acoustic samples from the same site one year (or more) apart prove more similar than two samples taken at the same time but from sites situated just a few hundred meters apart. These findings suggest that habitat modification can be heard as a long-lasting imprint on the soundscape of regenerating habitats and identify soundscape–area and soundscape–time relations as a promising tool for biodiversity research, applied biomonitoring and restoration ecology.},
  doi = {10.1111/ecog.04544},
  issn = {16000587},
  journal = {Ecography},
  mendeley-groups = {bioacoustic},
  number = {11},
  volume = {42},
  year = {2019},
}