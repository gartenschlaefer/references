# --
# bioacoustic reference collections

% bird, dialect
@article{Wang2022ZebraDialect,
  author = {Daiping Wang and Wolfgang Forstmeier and Damien R. Farine and Adriana A. Maldonado-Chaparro and Katrin Martin and Yifan Pei and Gustavo Alarcón-Nieto and James A. Klarevas-Irby and Shouwen Ma and Lucy M. Aplin and Bart Kempenaers},
  issue = {1},
  journal = {Nature Communications},
  title = {Machine learning reveals cryptic dialects that explain mate choice in a songbird},
  volume = {13},
  year = {2022},
  %issn = {20411723},
  %doi = {10.1038/s41467-022-28881-w},
  %abstract = {Culturally transmitted communication signals – such as human language or bird song – can change over time through cultural drift, and the resulting dialects may consequently enhance the separation of populations. However, the emergence of song dialects has been considered unlikely when songs are highly individual-specific, as in the zebra finch (Taeniopygia guttata). Here we show that machine learning can nevertheless distinguish the songs from multiple captive zebra finch populations with remarkable precision, and that ‘cryptic song dialects’ predict strong assortative mating in this species. We examine mating patterns across three consecutive generations using captive populations that have evolved in isolation for about 100 generations. We cross-fostered eggs within and between these populations and used an automated barcode tracking system to quantify social interactions. We find that females preferentially pair with males whose song resembles that of the females’ adolescent peers. Our study shows evidence that in zebra finches, a model species for song learning, individuals are sensitive to differences in song that have hitherto remained unnoticed by researchers.},
}

% individual animal vocalization, recognition
@article{Carlson2020IndivVocal,
  author = {Carlson, Nora V.  and Kelly, E. McKenna  and Couzin, Iain },
  title = {Individual vocal recognition across taxa: a review of the literature and a look into the future},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1802},
  pages = {20190479},
  year = {2020},
  %doi = {10.1098/rstb.2019.0479},
  %URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2019.0479},
  %eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2019.0479},
  %abstract = { Individual vocal recognition (IVR) has been well studied in mammals and birds. These studies have primarily delved into understanding IVR in specific limited contexts (e.g. parent–offspring and mate recognition) where individuals discriminate one individual from all others. However, little research has examined IVR in more socially demanding circumstances, such as when an individual discriminates all individuals in their social or familial group apart. In this review, we describe what IVR is and suggest splitting studies of IVR into two general types based on what questions they answer (IVR-singular, and IVR-multiple). We explain how we currently test for IVR, and many of the benefits and drawbacks of different methods. We address why IVR is so prevalent in the animal kingdom, and the circumstances in which it is often found. Finally, we explain current weaknesses in IVR research including temporality, specificity, and taxonomic bias, and testing paradigms, and provide some solutions to address these weaknesses. This article is part of the theme issue ‘Signal detection theory in recognition systems: from evolving models to experimental tests’. },
}

% software, avisoft, ultravox
@article{Binder2018Mice,
  author = {Matthew S. Binder and Christian J. Hernandez-Zegada and Christian T. Potter and Suzanne O. Nolan and Joaquin N. Lugo},
  title = {A comparison of the Avisoft (5.2) and Ultravox (2.0) recording systems: Implications for early-life communication and vocalization research},
  journal = {Journal of Neuroscience Methods},
  volume = {309},
  pages = {6-12},
  year = {2018},
  %issn = {0165-0270},
  %doi = {https://doi.org/10.1016/j.jneumeth.2018.08.015},
  %url = {https://www.sciencedirect.com/science/article/pii/S0165027018302516},
  %keywords = {Ultrasonic vocalizations, Autism, BL/6, 129SvEvTac, Neurodevelopment, Reproducibility},
  %abstract = {Alterations in early-life communicative behaviors are a common feature of neurodevelopmental conditions, such as autism and epilepsy. One method of investigating communication in murine models is through analyzing ultrasonic vocalizations. These vocalizations are commonly recorded with either the Avisoft or the Ultravox recording programs. However, since no study has compared whether the systems are equally sensitive, the findings in one program may not be reproducible in the other. To directly compare the two programs, we elicited vocalizations from male and female 129SvEvTac and C57BL/6 mouse pups via the maternal isolation paradigm, recording vocalizations simultaneously with both systems. We held the detection parameters identical for each system and found that there was only a medium correlation between Avisoft and Ultravox overall. Further analysis indicated that Avisoft detected more total vocalizations, as well as more vocalizations at the set frequencies of 50, 60, and 70 kHz than Ultravox, p <  .05. No statistically significant difference was present at 80 kHz. These findings demonstrate that different recording systems do not detect the same quantity of vocalizations as one another, even when detection parameters are congruent. Therefore, it may be useful to revisit previous negative results obtained with Ultravox and repeat the experiments using Avisoft. Ultimately, ultrasonic vocalizations are a valuable tool, capable of examining early-life phenotypes. However, a more thorough understanding of the relationships between recording systems is necessary to achieve a more comprehensive and reproducible assessment of vocalizing behaviors.}
}

% chicken distress calls, cnn, vgg11
@article{Mao2022ChickenStress,
  author = {Mao, Axiu  and Giraudet, Claire S. E.  and Liu, Kai  and De Almeida Nolasco, Inês  and Xie, Zhiqin  and Xie, Zhixun  and Gao, Yue  and Theobald, James  and Bhatta, Devaki  and Stewart, Rebecca  and McElligott, Alan G. },
  title = {Automated identification of chicken distress vocalizations using deep learning models},
  journal = {Journal of The Royal Society Interface},
  volume = {19},
  number = {191},
  pages = {20210921},
  year = {2022},
  %doi = {10.1098/rsif.2021.0921},
  %URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2021.0921},
  %eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2021.0921},
  %abstract = {The annual global production of chickens exceeds 25 billion birds, which are often housed in very large groups, numbering thousands. Distress calling triggered by various sources of stress has been suggested as an ‘iceberg indicator’ of chicken welfare. However, to date, the identification of distress calls largely relies on manual annotation, which is very labour-intensive and time-consuming. Thus, a novel convolutional neural network-based model, light-VGG11, was developed to automatically identify chicken distress calls using recordings (3363 distress calls and 1973 natural barn sounds) collected on an intensive farm. The light-VGG11 was modified from VGG11 with significantly fewer parameters (9.3 million versus 128 million) and 55.88\% faster detection speed while displaying comparable performance, i.e. precision (94.58\%), recall (94.89\%), F1-score (94.73\%) and accuracy (95.07\%), therefore more useful for model deployment in practice. To additionally improve light-VGG11's performance, we investigated the impacts of different data augmentation techniques (i.e. time masking, frequency masking, mixed spectrograms of the same class and Gaussian noise) and found that they could improve distress calls detection by up to 1.52\%. Our distress call detection demonstration on continuous audio recordings, shows the potential for developing technologies to monitor the output of this call type in large, commercial chicken flocks.},
}

% todo, ai and audio, sounds
@article{Schmidt2020AcousticAwareness,
  author={Schmidt, Alexander and Löllmann, Heinrich W. and Kellermann, Walter},
  title={Acoustic Self-Awareness of Autonomous Systems in a World of Sounds}, 
  journal={Proceedings of the IEEE}, 
  year={2020},
  volume={108},
  number={7},
  pages={1127-1149},
  %keywords={Acoustics;Robot sensing systems;Image analysis;Microphones;Signal processing algorithms;Autonomous systems;Loudspeakers;Acoustic scene analysis (ASA);active enhancement;active localization;autonomous systems (ASs);ego-noise;exploration;self-awareness},
  %doi={10.1109/JPROC.2020.2977372},
}

% bee, detection, system
@article{Kiromitis2022Bee,
  author = {Kiromitis, Dimitrios I. and Bellos, Christos V. and Stefanou, Konstantinos A. and Stergios, Georgios S. and Katsantas, Thomas and Kontogiannis, Sotirios},
  title = {Bee Sound Detector: An Easy-to-Install, Low-Power, Low-Cost Beehive Conditions Monitoring System},
  journal = {Electronics},
  volume = {11},
  year = {2022},
  number = {19},
  %URL = {https://www.mdpi.com/2079-9292/11/19/3152},
  %ISSN = {2079-9292},
  %ABSTRACT = {One of the most significant agricultural tasks in beekeeping involves continually observing the conditions inside and outside the beehive. This is mainly performed for the early detection of some harmful events. There have been many studies on how to detect and prevent such occurrences by performing periodic interventions or, when the frequency of such actions is hard to enforce, by using sensory systems that record the temperature, humidity, and weight of the beehive. Nevertheless, such methods are inaccurate, and their delivered outcomes usually diverge from the actual event or false trigger and introduce more effort and damage. In this paper, the authors propose a new low-cost, low-power system called Bee Sound Detector (BeeSD). BeeSD is a low-cost, embedded solution for beehive quality control. It incorporates the sensors mentioned above as well as real-time sound monitoring. With the combination of temperature, humidity, and sound sensors, the BeeSD can spot Colony Collapse Disorder events due to famine and extreme weather events, queen loss, and swarming. Furthermore, as a system, the BeeSD uses cloud logging and an appropriate mobile phone application to push notifications of extreme measurements to the farmers. Based on achieved performance indicators, the authors present their BeeSD IoT device and system operation, focusing on its advantages of low-cost, low-power, and easy-to-install characteristics.},
  %DOI = {10.3390/electronics11193152},
}

% bee
@inproceedings{Nolasco2019Bee,
  author={Nolasco, Ines and Terenzi, Alessandro and Cecchi, Stefania and Orcioni, Simone and Bear, Helen L. and Benetos, Emmanouil},
  title={Audio-based Identification of Beehive States}, 
  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  year={2019},
  volume={},
  number={},
  pages={8256-8260},
  %keywords={Feature extraction;Training;Support vector machines;Transforms;Spectrogram;Image analysis;Stress;Empirical mode decomposition;Hilbert-Huang transform;bioacoustics;computational bioacoustic scene analysis;beehive state recognition},
  %doi={10.1109/ICASSP.2019.8682981},
}

% bird, deep learning
@article{Xie2023Bird,
  author = {Xie, Jie and Zhu, Mingying},
  title = {Acoustic Classification of Bird Species Using an Early Fusion of Deep Features},
  journal = {Birds},
  volume = {4},
  year = {2023},
  number = {1},
  pages = {138--147},
  %URL = {https://www.mdpi.com/2673-6004/4/1/11},
  %ISSN = {2673-6004},
  %ABSTRACT = {Bird sound classification plays an important role in large-scale temporal and spatial environmental monitoring. In this paper, we investigate both transfer learning and training from scratch for bird sound classification, where pre-trained models are used as feature extractors. Specifically, deep cascade features are extracted from various layers of different pre-trained models, which are then fused to classify bird sounds. A multi-view spectrogram is constructed to characterize bird sounds by simply repeating the spectrogram to make it suitable for pre-trained models. Furthermore, both mixup and pitch shift are applied for augmenting bird sounds to improve the classification performance. Experimental classification on 43 bird species using linear SVM indicates that deep cascade features can achieve the highest balanced accuracy of 90.94% ± 1.53%. To further improve the classification performance, an early fusion method is used by combining deep cascaded features extracted from different pre-trained models. The final best classification balanced accuracy is 94.89% ± 1.35%.},
  %DOI = {10.3390/birds4010011},
}

% calve coughs
@article{Vandermeulen2016CalveCough,
  author = {Joris Vandermeulen and Claudia Bahr and Dayle Johnston and Bernadette Earley and Emanuela Tullo and Ilaria Fontana and Marcella Guarino and Vasileios Exadaktylos and Daniel Berckmans},
  title = {Early recognition of bovine respiratory disease in calves using automated continuous monitoring of cough sounds},
  journal = {Computers and Electronics in Agriculture},
  volume = {129},
  pages = {15-26},
  year = {2016},
  %issn = {0168-1699},
  %doi = {https://doi.org/10.1016/j.compag.2016.07.014},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168169916305105},
  %keywords = {Disease monitoring, Sound analysis, Bioacoustics, Non-invasive technology},
  %abstract = {Bovine respiratory disease (BRD) complex in calves impairs health and welfare and causes severe economic losses for the Stockperson. Early recognition of BRD should lead to earlier veterinary (antibiotic/anti-inflammatory) treatment interventions thereby reducing the severity of the disease and associated costs. Coughing is one of the clinical manifestations of BRD. It is believed that by automatically and continuously monitoring the sounds within calf houses, and analysing the coughing frequency, early recognition of BRD in calves is possible. Therefore, the objective of the present study was to develop an automated calf cough monitor and examine its potential as an early warning system for BRD in artificially reared dairy calves. The coughing sounds of 62 calves were continuously recorded by a microphone over a three-month period. A sound analysis algorithm was developed to distinguish calf coughs from other sounds (e.g. mechanical sounds). During the sound recording period the health of the calves was assessed and scored periodically per week by a trained human observer. Calves presenting with BRD received antibiotic and/or anti-inflammatory treatment and the dates of treatment were recorded. This treatment date reference served as a comparison for the investigation of whether an increase in coughing frequency could be related to calves developing BRD. The calf cough detection algorithm achieved 50.3% sensitivity, 99.2% specificity and 87.5% precision. Four out of five periods, where coughing frequency was observed to be increased, coincided with the development of BRD in more than one calf. This period of increased coughing frequency was always observed before the calves were treated. Therefore, the calf cough monitor has the potential to identify early onset of BRD in calves.},
}

% pig coughs
@article{Exadaktylos2008PigCough,
  author = {V. Exadaktylos and M. Silva and J.-M. Aerts and C.J. Taylor and D. Berckmans},
  title = {Real-time recognition of sick pig cough sounds},
  journal = {Computers and Electronics in Agriculture},
  volume = {63},
  number = {2},
  pages = {207-214},
  year = {2008},
  %issn = {0168-1699},
  %doi = {https://doi.org/10.1016/j.compag.2008.02.010},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168169908000835},
  %keywords = {Real-time recognition, Cough analysis, Pig vocalisations, Biomedical system, Spectral analysis},
  %abstract = {This paper extends existing cough identification methods and proposes a real-time method for identifying sick pig cough sounds. The analysis and classification is based on the frequency domain characteristics of the signal, while an improved procedure to extract the reference is presented. This technique evaluates fuzzy c-means clustering to parts of the training signals and provides a frequency content reference that mirrors the characteristics of sick pig cough. The extraction of the reference is performed in such a way that allows for the identification process to be implemented in real-time applications that would speed up the diagnosis and treatment process and improve animal welfare in pig houses. Preliminary results for the evaluation of the algorithm are based on individual sounds of healthy and sick animals acquired in laboratory conditions. An 85% overall correct classification ratio is achieved with 82% of the sick cough sounds being correctly identified.},
}

% individual, identification
@article{Stowell2019Individuals,
  author = {Stowell, Dan  and Petrusková, Tereza  and Šálek, Martin  and Linhart, Pavel },
  title = {Automatic acoustic identification of individuals in multiple species: improving identification across recording conditions},
  journal = {Journal of The Royal Society Interface},
  volume = {16},
  number = {153},
  pages = {20180940},
  year = {2019},
  %doi = {10.1098/rsif.2018.0940},
  %URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2018.0940},
  %eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2018.0940},
  %abstract = { Many animals emit vocal sounds which, independently from the sounds’ function, contain some individually distinctive signature. Thus the automatic recognition of individuals by sound is a potentially powerful tool for zoology and ecology research and practical monitoring. Here, we present a general automatic identification method that can work across multiple animal species with various levels of complexity in their communication systems. We further introduce new analysis techniques based on dataset manipulations that can evaluate the robustness and generality of a classifier. By using these techniques, we confirmed the presence of experimental confounds in situations resembling those from past studies. We introduce data manipulations that can reduce the impact of these confounds, compatible with any classifier. We suggest that assessment of confounds should become a standard part of future studies to ensure they do not report over-optimistic results. We provide annotated recordings used for analyses along with this study and we call for dataset sharing to be a common practice to enhance the development of methods and comparisons of results.},
}

% biodiversity, metrics, entropy
@article{Sueur2009,
  author = {Sueur, Jérôme AND Pavoine, Sandrine AND Hamerlynck, Olivier AND Duvail, Stéphanie},
  title = {Rapid Acoustic Survey for Biodiversity Appraisal},
  journal = {PLOS ONE},
  publisher = {Public Library of Science},
  volume = {3},
  number = {12},
  pages = {1-9},
  month = {12},
  year = {2009},
  %url = {https://doi.org/10.1371/journal.pone.0004065},
  %doi = {10.1371/journal.pone.0004065},
  %abstract = {Biodiversity assessment remains one of the most difficult challenges encountered by ecologists and conservation biologists. This task is becoming even more urgent with the current increase of habitat loss. Many methods–from rapid biodiversity assessments (RBA) to all-taxa biodiversity inventories (ATBI)–have been developed for decades to estimate local species richness. However, these methods are costly and invasive. Several animals–birds, mammals, amphibians, fishes and arthropods–produce sounds when moving, communicating or sensing their environment. Here we propose a new concept and method to describe biodiversity. We suggest to forego species or morphospecies identification used by ATBI and RBA respectively but rather to tackle the problem at another evolutionary unit, the community level. We also propose that a part of diversity can be estimated and compared through a rapid acoustic analysis of the sound produced by animal communities. We produced α and β diversity indexes that we first tested with 540 simulated acoustic communities. The α index, which measures acoustic entropy, shows a logarithmic correlation with the number of species within the acoustic community. The β index, which estimates both temporal and spectral dissimilarities, is linearly linked to the number of unshared species between acoustic communities. We then applied both indexes to two closely spaced Tanzanian dry lowland coastal forests. Indexes reveal for this small sample a lower acoustic diversity for the most disturbed forest and acoustic dissimilarities between the two forests suggest that degradation could have significantly decreased and modified community composition. Our results demonstrate for the first time that an indicator of biological diversity can be reliably obtained in a non-invasive way and with a limited sampling effort. This new approach may facilitate the appraisal of animal diversity at large spatial and temporal scales.},
}

% ecology, pam, monitoring
@article{Ross2023PAMEcoQuestions,
  author = {Ross, Samuel R. P.-J. and O'Connell, Darren P. and Deichmann, Jessica L. and Desjonquères, Camille and Gasc, Amandine and Phillips, Jennifer N. and Sethi, Sarab S. and Wood, Connor M. and Burivalova, Zuzana},
  title = {Passive acoustic monitoring provides a fresh perspective on fundamental ecological questions},
  journal = {Functional Ecology},
  volume = {37},
  number = {4},
  pages = {959-975},
  year = {2023},
  %doi = {https://doi.org/10.1111/1365-2435.14275},
  %url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2435.14275},
  %eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2435.14275},
  %keywords = {acoustic index, bioacoustics, biodiversity, ecoacoustics, sensor array, soundscape, spatiotemporal scale},
  %abstract = {Abstract Passive acoustic monitoring (PAM) has emerged as a transformative tool for applied ecology, conservation and biodiversity monitoring, but its potential contribution to fundamental ecology is less often discussed, and fundamental PAM studies tend to be descriptive, rather than mechanistic. Here, we chart the most promising directions for ecologists wishing to use the suite of currently available acoustic methods to address long-standing fundamental questions in ecology and explore new avenues of research. In both terrestrial and aquatic habitats, PAM provides an opportunity to ask questions across multiple spatial scales and at fine temporal resolution, and to capture phenomena or species that are difficult to observe. In combination with traditional approaches to data collection, PAM could release ecologists from myriad limitations that have, at times, precluded mechanistic understanding. We discuss several case studies to demonstrate the potential contribution of PAM to biodiversity estimation, population trend analysis, assessing climate change impacts on phenology and distribution, and understanding disturbance and recovery dynamics. We also highlight what is on the horizon for PAM, in terms of near-future technological and methodological developments that have the potential to provide advances in coming years. Overall, we illustrate how ecologists can harness the power of PAM to address fundamental ecological questions in an era of ecology no longer characterised by data limitation. Read the free Plain Language Summary for this article on the Journal blog.},
}

% biodiversity, climate change, ecoacoustics
@article{Krause2016EcoacousticClimate,
  author = {Bernie Krause and Almo Farina},
  title = {Using ecoacoustic methods to survey the impacts of climate change on biodiversity},
  journal = {Biological Conservation},
  volume = {195},
  pages = {245-254},
  year = {2016},
  %issn = {0006-3207},
  %doi = {https://doi.org/10.1016/j.biocon.2016.01.013},
  %url = {https://www.sciencedirect.com/science/article/pii/S0006320716300118},
  %keywords = {Climate change, Biodiversity conservation, Ecoacoustics, Acoustic adaptation hypothesis, Acoustic niche hypothesis, Acoustic active space, Acoustic community, Acoustic phenology},
  %abstract = {Climate change is an important cause of the irreversible transformation of habitats, of the rapid extinction of species, and of the dramatic changes in entire communities, especially for tropical assemblages and for habitat- and range-restricted species, such as mountaintop and polar species. In particular, climate change effects several aspects of animal sounds (e.g., song amplitude and frequency, song post, and sound phenology). Animal sounds, which are life traits characterized by high plasticity, are able to cope with even modest variations of environmental fundamentals like vegetation cover, land mosaic structure, temperature, humidity, and pH (for aquatic medium). Moreover, the climatic effects on these biophonies can be observed earlier than change in vegetation patterns and visible landscape structures. Ecoacoustics, the discipline that investigates the role of sound on animal ecology from species to landscapes, offers robust models, such as acoustic adaptation, acoustic niche, acoustic active space, acoustic community, and acoustic phenology to investigate the effect of climate change on species, populations, communities, and landscapes. From an operational perspective, ecoacoustics procedures can be applied in different contexts, such as locations, weather, species, populations, behavior, physiology, and phenology. In addition, thematic priorities can be selected, such as latitudinal and altitudinal gradients, restricted habitats, stopover areas, extreme environments, weather conditions, short distance migrants, species at high vocal plasticity, sink-source status, active space, social attraction, physiological modifications, dawn and dusk choruses, sound from stressed plants, and time series analysis. The noninvasiveness of passive acoustic recording, the simultaneous collection of important data, such as community richness and diversity, immigration and extinction events, and singing dynamics as well as the availability of innovative noninvasive technologies operating over a long-term period, establish ecoacoustics as a new and important tool with which it is possible to analyze massive acoustic data sets and quickly predict and/or evaluate the effects of climate change on the environment. Moreover, passive recording is supported by cheap, user-friendly field sensors and robust data processing and may be part of the citizen science research agenda on climate change.},
}

% marine, animal welfare, monitoring
@article{Winship2023Marine,
  author = {Winship, Kelley A. and Jones, Brittany L.},
  title = {Acoustic Monitoring of Professionally Managed Marine Mammals for Health and Welfare Insights},
  journal = {Animals},
  volume = {13},
  number = {13},
  year = {2023},
  %URL = {https://www.mdpi.com/2076-2615/13/13/2124},
  %PubMedID = {37443922},
  %ISSN = {2076-2615},
  %DOI = {10.3390/ani13132124},
  %ABSTRACT = {Research evaluating marine mammal welfare and opportunities for advancements in the care of species housed in a professional facility have rapidly increased in the past decade. While topics, such as comfortable housing, adequate social opportunities, stimulating enrichment, and a high standard of medical care, have continued to receive attention from managers and scientists, there is a lack of established acoustic consideration for monitoring the welfare of these animals. Marine mammals rely on sound production and reception for navigation and communication. Regulations governing anthropogenic sound production in our oceans have been put in place by many countries around the world, largely based on the results of research with managed and trained animals, due to the potential negative impacts that unrestricted noise can have on marine mammals. However, there has not been an established best practice for the acoustic welfare monitoring of marine mammals in professional care. By monitoring animal hearing and vocal behavior, a more holistic view of animal welfare can be achieved through the early detection of anthropogenic sound sources, the acoustic behavior of the animals, and even the features of the calls. In this review, the practice of monitoring cetacean acoustic welfare through behavioral hearing tests and auditory evoked potentials (AEPs), passive acoustic monitoring, such as the Welfare Acoustic Monitoring System (WAMS), as well as ideas for using advanced technologies for utilizing vocal biomarkers of health are introduced and reviewed as opportunities for integration into marine mammal welfare plans.},
}

% bonobo, animal communications
@article{Arnaud2023Bonobo,
  author = {Arnaud, Vincent and Pellegrino, François and Keenan, Sumir and St-Gelais, Xavier and Mathevon, Nicolas and Levréro, Florence and Coupé, Christophe},
  title = {Improving the workflow to crack Small, Unbalanced, Noisy, but Genuine (SUNG) datasets in bioacoustics: The case of bonobo calls},
  journal = {PLOS Computational Biology},
  publisher = {Public Library of Science},
  volume = {19},
  pages = {1-47},
  number = {4},
  month = {04},
  year = {2023},
  %url = {https://doi.org/10.1371/journal.pcbi.1010325},
  %doi = {10.1371/journal.pcbi.1010325},
  %abstract = {Despite the accumulation of data and studies, deciphering animal vocal communication remains challenging. In most cases, researchers must deal with the sparse recordings composing Small, Unbalanced, Noisy, but Genuine (SUNG) datasets. SUNG datasets are characterized by a limited number of recordings, most often noisy, and unbalanced in number between the individuals or categories of vocalizations. SUNG datasets therefore offer a valuable but inevitably distorted vision of communication systems. Adopting the best practices in their analysis is essential to effectively extract the available information and draw reliable conclusions. Here we show that the most recent advances in machine learning applied to a SUNG dataset succeed in unraveling the complex vocal repertoire of the bonobo, and we propose a workflow that can be effective with other animal species. We implement acoustic parameterization in three feature spaces and run a Supervised Uniform Manifold Approximation and Projection (S-UMAP) to evaluate how call types and individual signatures cluster in the bonobo acoustic space. We then implement three classification algorithms (Support Vector Machine, xgboost, neural networks) and their combination to explore the structure and variability of bonobo calls, as well as the robustness of the individual signature they encode. We underscore how classification performance is affected by the feature set and identify the most informative features. In addition, we highlight the need to address data leakage in the evaluation of classification performance to avoid misleading interpretations. Our results lead to identifying several practical approaches that are generalizable to any other animal communication system. To improve the reliability and replicability of vocal communication studies with SUNG datasets, we thus recommend: i) comparing several acoustic parameterizations; ii) visualizing the dataset with supervised UMAP to examine the species acoustic space; iii) adopting Support Vector Machines as the baseline classification approach; iv) explicitly evaluating data leakage and possibly implementing a mitigation strategy.},
}

% chicken, monitoring
@article{Mahdavian2021ChickenMonitoring,
  author = {Alireza Mahdavian and Saeid Minaei and Peter M. Marchetto and Farshad Almasganj and Shaban Rahimi and Ce Yang},
  title = {Acoustic features of vocalization signal in poultry health monitoring},
  journal = {Applied Acoustics},
  volume = {175},
  pages = {107756},
  year = {2021},
  %issn = {0003-682X},
  %doi = {https://doi.org/10.1016/j.apacoust.2020.107756},
  %url = {https://www.sciencedirect.com/science/article/pii/S0003682X20308616},
  %keywords = {Bioacoustics, Health monitoring, Respiratory diseases, Audio features, Precision livestock farming},
  %abstract = {In precision livestock farming, studies show that bird sound can be employed as a biomarker of health condition. One of the most important steps for this purpose is to study the feasibility of using acoustic features as criteria for disease diagnosis. In this research five acoustic features of bird calls were evaluated for determination of bird health condition. Signals were collected from broilers grown in three groups: control, challenged with Bronchitis, and challenged with Newcastle disease. Results of data analysis showed that, among the 5 acoustic features studied, wavelet entropy (WET) had the best performance and was able to detect Bronchitis on the third day after inoculation with 83% accuracy while the type II error in this test (incorrectly detecting sick bird as healthy) was less than 14% and 6% on the third day and fourth day, respectively. In the case of Newcastle disease, although WET and Mel cepstral coefficients (MFCC) exhibited similar accuracy (80% and 78% respectively on the fourth day), but the difference was that WET was more reliable in detecting healthy birds while MFCC had better performance detecting challenged birds.},
}

% pig, welfare
@article{Cordeiro2018PigVoc,
  author = {Alexandra F.da S. Cordeiro and Irenilza de A. Nääs and Felipe {da Silva Leitão} and Andréia C.M. {de Almeida} and Daniella Jorge {de Moura}},
  title = {Use of vocalisation to identify sex, age, and distress in pig production},
  journal = {Biosystems Engineering},
  volume = {173},
  pages = {57-63},
  year = {2018},
  %note = {Advances in the Engineering of Sensor-based Monitoring and Management Systems for Precision Livestock Farming},
  %issn = {1537-5110},
  %doi = {https://doi.org/10.1016/j.biosystemseng.2018.03.007},
  %url = {https://www.sciencedirect.com/science/article/pii/S153751101730435X},
  %keywords = {Pig welfare, Acoustic signals, Pig production, Machine learning},
  %abstract = {To assess animal welfare at a pig production farm is a time-consuming task. The present study aimed to investigate the differences in pig vocalisation as a function of the sex, age, and distress conditions, and to propose a way of identifying distressful situations. The individual vocalisations of 40 pigs were recorded (20 male and 20 female) during exposure to different distress in the farrowing, nursery, growth, and finishing phases. Vocalisation pitch differed between males (194.5 Hz) and females (218.2 Hz). Pig vocalisation was also different according to age, especially for the attributes of maximum and minimum amplitudes, and the frequency of formant 2. Diverse distress situations also were identified by various acoustic attributes. A decision-tree for classifying the distress condition for pigs was built (with an accuracy of 81.92%) using the machine-learning technique. Results indicate the possibility of estimating pig welfare by recording the vocalisation. The algorithm is also promising to identify pig sex and age.},
}

% cow, monitoring, plf
@article{Jung2021DLCow,
  author = {Jung, Dae-Hyun and Kim, Na Yeon and Moon, Sang Ho and Jhin, Changho and Kim, Hak-Jin and Yang, Jung-Seok and Kim, Hyoung Seok and Lee, Taek Sung and Lee, Ju Young and Park, Soo Hyun},
  title = {Deep Learning-Based Cattle Vocal Classification Model and Real-Time Livestock Monitoring System with Noise Filtering},
  journal = {Animals},
  volume = {11},
  year = {2021},
  number = {2},
  %article-number = {357},
  %url = {https://www.mdpi.com/2076-2615/11/2/357},
  %PubMedID = {33535390},
  %issn = {2076-2615},
  %doi = {10.3390/ani11020357},
  %abstract = {The priority placed on animal welfare in the meat industry is increasing the importance of understanding livestock behavior. In this study, we developed a web-based monitoring and recording system based on artificial intelligence analysis for the classification of cattle sounds. The deep learning classification model of the system is a convolutional neural network (CNN) model that takes voice information converted to Mel-frequency cepstral coefficients (MFCCs) as input. The CNN model first achieved an accuracy of 91.38% in recognizing cattle sounds. Further, short-time Fourier transform-based noise filtering was applied to remove background noise, improving the classification model accuracy to 94.18%. Categorized cattle voices were then classified into four classes, and a total of 897 classification records were acquired for the classification model development. A final accuracy of 81.96% was obtained for the model. Our proposed web-based platform that provides information obtained from a total of 12 sound sensors provides cattle vocalization monitoring in real time, enabling farm owners to determine the status of their cattle.},
}

% cow, monitoring, welfare
@article{Meen2015SoundAnalysisCattleWelfare,
  author = {G.H. Meen and M.A. Schellekens and M.H.M. Slegers and N.L.G. Leenders and E. {van Erp-van der Kooij} and L.P.J.J. Noldus},
  title = {Sound analysis in dairy cattle vocalisation as a potential welfare monitor},
  journal = {Computers and Electronics in Agriculture},
  volume = {118},
  pages = {111-115},
  year = {2015},
  %issn = {0168-1699},
  %doi = {https://doi.org/10.1016/j.compag.2015.08.028},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168169915002549},
  %keywords = {Precision livestock farming, Sound analysis, Dairy cattle, Behaviour, Video analysis, Audio analysis},
  %abstract = {In modern farming there is a growing demand for innovative tools gathering and analysing information concerning the herd, as well as individual animals. In Precision Livestock Farming (PLF), technology continuously measures various variables as activity, food intake or oestrus activity, thereby supporting farmers in monitoring his livestock. Sound analysis has shown to be useful as an early warning tool in pigs and it is unknown whether sound analysis can also be applied in cattle. Goal of this research was to determine whether a correlation can be found between cattle vocalisation and cattle behaviour. The vocalisations and behaviour of Holstein Friesian cattle were observed using audio and video recordings. Four cameras and four microphones were installed at a high production dairy farm in Herwijnen, the Netherlands. Three sets (a set consisting of both a camera and a microphone) recorded dairy cattle between two and fourteen years of age, one set recorded heifers between four and ten months of age. Recordings were made for fifteen days in three consecutive weeks, ten hours per day. Calls of cattle were traced to an individual cow and, if possible, linked with simultaneously expressed behaviour. The used ethogram consisted of six behavioural groups: lying & ruminating, feeding related behaviour, social interaction, sexual related behaviour, stress related behaviour and remaining behaviour. Lying & ruminating was a separate class since this behaviour expresses the needs of a cow. The maximum frequency in Hertz (Hz) of each call was determined. Statistical analysis showed a significant difference between the mean maximum frequency (Hz) of calls during lying & ruminating and calls recorded during other behaviours (83±4.3Hz versus 298±8.0Hz; p<0.05). Calls by adult dairy cattle had a significantly lower maximum frequency (Hz) than calls by heifers. (332.6±0.2Hz versus 218.5Hz±0.3Hz; p<0.05). This study may provide a foothold towards the use of sound analysis as a tool for dairy cattle management. If calls by cattle can be used to monitor welfare, dairy farmers can be alerted when cattle welfare is decreasing.},
}

% sperm whale, technical, contrastive learning
@article {Bermant2022CLWhale,
  author = {Peter C. Bermant and Leandra Brickson and Alexander J. Titus},
  title = {Bioacoustic Event Detection with Self-Supervised Contrastive Learning},
  year = {2022},
  publisher = {Cold Spring Harbor Laboratory},
  journal = {bioRxiv},
  %doi = {10.1101/2022.10.12.511740},
  %elocation-id = {2022.10.12.511740},
  %URL = {https://www.biorxiv.org/content/early/2022/10/16/2022.10.12.511740.1},
  %eprint = {https://www.biorxiv.org/content/early/2022/10/16/2022.10.12.511740.1.full.pdf},
  %abstract = {While deep learning has revolutionized ecological data analysis, existing strategies often rely on supervised learning, which is subject to limitations on real-world applicability. In this paper, we apply self-supervised deep learning methods to bioacoustic data to enable unsupervised detection of bioacoustic event boundaries. We propose a convolutional deep neural network that operates on the raw waveform directly and is trained in accordance with the Noise Contrastive Estimation principle, which enables the system to detect spectral changes in the input acoustic stream. The model learns a representation of the input audio sampled at low frequency that encodes information regarding dissimilarity between sequential acoustic windows. During inference, we use a peak finding algorithm to search for regions of high dissimilarity in order to identify temporal boundaries of bioacoustic events. We report results using these techniques to detect sperm whale (Physeter macrocephalus) coda clicks in real-world recordings, and we demonstrate the viability of analyzing the vocalizations of other species (e.g. Bengalese finch syllable segmentation) in addition to other data modalities (e.g. animal behavioral dynamics, embryo development and tracking). We find that the self-supervised deep representation learning-based technique outperforms established threshold-based baseline methods without requiring manual annotation of acoustic datasets. Quantitatively, our approach yields a maximal R-value and F1-score of 0.887 and 0.876, respectively, and an area under the Precision-Recall curve (PR-AUC) of 0.917, while a baseline threshold detector acting on signal energy amplitude returns a maximal R-value and F1-score of 0.620 and 0.576, respectively, and a PR-AUC of 0.571. We also compare with a threshold detector using preprocessed (e.g. denoised) acoustic input. The findings of this paper establish the validity of unsupervised bioacoustic event detection using deep neural networks and self-supervised contrastive learning as an effective alternative to conventional techniques that leverage supervised methods for signal presence indication. Providing a means for highly accurate unsupervised detection, this paper serves as an important step towards developing a fully automated system for real-time acoustic monitoring of bioacoustic signals in real-world acoustic data. All code and data used in this study are available online.Competing Interest StatementThe authors have declared no competing interest.},
}

% monitoring, survey
@article{Gibb2019EcologicalAssesment,
  author = {Gibb, Rory and Browning, Ella and Glover-Kapfer, Paul and Jones, Kate E.},
  title = {Emerging opportunities and challenges for passive acoustics in ecological assessment and monitoring},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {2},
  pages = {169-185},
  year = {2019},
  %keywords = {acoustic indices, bioacoustics, biodiversity monitoring, deep learning, ecoacoustics, ecological monitoring, machine learning, passive acoustic monitoring},
  %doi = {https://doi.org/10.1111/2041-210X.13101},
  %url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13101},
  %eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13101},
  %abstract = {Abstract High-throughput environmental sensing technologies are increasingly central to global monitoring of the ecological impacts of human activities. In particular, the recent boom in passive acoustic sensors has provided efficient, noninvasive, and taxonomically broad means to study wildlife populations and communities, and monitor their responses to environmental change. However, until recently, technological costs and constraints have largely confined research in passive acoustic monitoring (PAM) to a handful of taxonomic groups (e.g., bats, cetaceans, birds), often in relatively small-scale, proof-of-concept studies. The arrival of low-cost, open-source sensors is now rapidly expanding access to PAM technologies, making it vital to evaluate where these tools can contribute to broader efforts in ecology and biodiversity research. Here, we synthesise and critically assess the current emerging opportunities and challenges for PAM for ecological assessment and monitoring of both species populations and communities. We show that terrestrial and marine PAM applications are advancing rapidly, facilitated by emerging sensor hardware, the application of machine learning innovations to automated wildlife call identification, and work towards developing acoustic biodiversity indicators. However, the broader scope of PAM research remains constrained by limited availability of reference sound libraries and open-source audio processing tools, especially for the tropics, and lack of clarity around the accuracy, transferability and limitations of many analytical methods. In order to improve possibilities for PAM globally, we emphasise the need for collaborative work to develop standardised survey and analysis protocols, publicly archived sound libraries, multiyear audio datasets, and a more robust theoretical and analytical framework for monitoring vocalising animal communities.},
}

% monitoring, survey, literature review
@inproceedings{Sharma2022BioAcMonitoring,
  author={Sharma, Sandhya and Sato, Kazuhiko and Gautam, Bishnu Prasad},
  title={Bioacoustics Monitoring of Wildlife using Artificial Intelligence: A Methodological Literature Review}, 
  booktitle={2022 International Conference on Networking and Network Applications (NaNA)}, 
  year={2022},
  volume={},
  number={},
  pages={329-337},
  %keywords={Social networking (online);Biological system modeling;Bibliographies;Wildlife;Prediction algorithms;Classification algorithms;Spatiotemporal phenomena;Artificial Intelligence;bioacoustics;monitoring;review;wildlife},
  %doi={10.1109/NaNA56854.2022.00063},
}

% identify animal species, asi, monitoring
@article{Ovaskainen2018ASI,
  author = {Ovaskainen, Otso and Moliterno de Camargo, Ulisses and Somervuo, Panu},
  title = {Animal Sound Identifier (ASI): software for automated identification of vocal animals},
  journal = {Ecology Letters},
  volume = {21},
  number = {8},
  pages = {1244-1254},
  year = {2018},
  %keywords = {Automated vocal identification, autonomous audio recording, joint species distribution modelling, species classification, species identification, vocal communities},
  %doi = {https://doi.org/10.1111/ele.13092},
  %url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.13092},
  %eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ele.13092},
  %abstract = {Abstract Automated audio recording offers a powerful tool for acoustic monitoring schemes of bird, bat, frog and other vocal organisms, but the lack of automated species identification methods has made it difficult to fully utilise such data. We developed Animal Sound Identifier (ASI), a MATLAB software that performs probabilistic classification of species occurrences from field recordings. Unlike most previous approaches, ASI locates training data directly from the field recordings and thus avoids the need of pre-defined reference libraries. We apply ASI to a case study on Amazonian birds, in which we classify the vocalisations of 14 species in 194 504 one-minute audio segments using in total two weeks of expert time to construct, parameterise, and validate the classification models. We compare the classification performance of ASI (with training templates extracted automatically from field data) to that of monitoR (with training templates extracted manually from the Xeno-Canto database), the results showing ASI to have substantially higher recall and precision rates.},
}

% soundscape, biodiversity
@article{Sethi2023LimitsScBiodiv,
  author = {Sarab S. Sethi and Avery Bick and Robert M. Ewers and Holger Klinck and Vijay Ramesh and Mao Ning Tuanmu and David A. Coomes},
  title = {Limits to the accurate and generalizable use of soundscapes to monitor biodiversity},
  journal = {Nature Ecology and Evolution},
  issue = {9},
  volume = {7},
  year = {2023},
  %doi = {10.1038/s41559-023-02148-z},
  %issn = {2397334X},
  %abstract = {Although eco-acoustic monitoring has the potential to deliver biodiversity insight on vast scales, existing analytical approaches behave unpredictably across studies. We collated 8,023 audio recordings with paired manual avifaunal point counts to investigate whether soundscapes could be used to monitor biodiversity across diverse ecosystems. We found that neither univariate indices nor machine learning models were predictive of species richness across datasets but soundscape change was consistently indicative of community change. Our findings indicate that there are no common features of biodiverse soundscapes and that soundscape monitoring should be used cautiously and in conjunction with more reliable in-person ecological surveys.},
}

% soundscape, encoding
@article{Sethi2020SoundscapeFeature,
  author = {Sarab S. Sethi  and Nick S. Jones  and Ben D. Fulcher  and Lorenzo Picinali  and Dena Jane Clink  and Holger Klinck  and C. David L. Orme  and Peter H. Wrege  and Robert M. Ewers },
  title = {Characterizing soundscapes across diverse ecosystems using a universal acoustic feature set},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {29},
  pages = {17049-17055},
  year = {2020},
  %doi = {10.1073/pnas.2004702117},
  %URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2004702117},
  %eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2004702117},
  %abstract = {Natural habitats are being impacted by human pressures at an alarming rate. Monitoring these ecosystem-level changes often requires labor-intensive surveys that are unable to detect rapid or unanticipated environmental changes. Here we have developed a generalizable, data-driven solution to this challenge using eco-acoustic data. We exploited a convolutional neural network to embed soundscapes from a variety of ecosystems into a common acoustic space. In both supervised and unsupervised modes, this allowed us to accurately quantify variation in habitat quality across space and in biodiversity through time. On the scale of seconds, we learned a typical soundscape model that allowed automatic identification of anomalous sounds in playback experiments, providing a potential route for real-time automated detection of irregular environmental behavior including illegal logging and hunting. Our highly generalizable approach, and the common set of features, will enable scientists to unlock previously hidden insights from acoustic data and offers promise as a backbone technology for global collaborative autonomous ecosystem monitoring efforts.},
}

% poultry, chicken
@article{Mahdavian2020PoultryCalls,
  author = {Alireza Mahdavian and Saeid Minaei and Ce Yang and Farshad Almasganj and Shaban Rahimi and Peter M. Marchetto},
  title = {Ability evaluation of a voice activity detection algorithm in bioacoustics: A case study on poultry calls},
  journal = {Computers and Electronics in Agriculture},
  volume = {168},
  pages = {105100},
  year = {2020},
  %issn = {0168-1699},
  %doi = {https://doi.org/10.1016/j.compag.2019.105100},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168169918305982},
  %keywords = {Bioacoustics, Health monitoring, Respiratory diseases, Audio features, },
  %abstract = {Poultry is one of the most strategic source of human foods. There have been seen some hopeful signs of bioacoustics application to monitor the health condition of this vital food source. One of the obstacles is that the bird’s call is combined with some unvoiced sounds and extracting the calls is not easy, especially when the bird is sick. This research is a report on successful application of some of the features involved in extracting healthy and non-healthy birds’ calls from their sound signals. One hundred and twenty birds from two genotypes – Ross and Cobb – were placed in two groups, a control and those challenged with respiratory diseases. They were reared and their sound was recorded daily. The vocal phrases of the recorded audio signals were extracted using the presented algorithm. Results of analysis showed that an increase in age and onset of illness are two factors that cause an error increase. Detection accuracy was calculated at 95% for healthy young birds and 72% for non-healthy birds. A significant part of this error is due to misclassing the calls as non-vocal segments. This meant that 97% of the activities classified as vocal phrases were, in fact, vocal. These results showed that the idea of such an easy-to-implement algorithm could potentially be employed for the coarselevel segmentation of some animal vocalization signals with reliable outputs, which is an essential and primary step in bioacoustics research.},
}

% contrastive
@inproceedings{Moummad2023CL,
  author={Ilyass Moummad and Romain Serizel and Nicolas Farrugia},
  title={Pretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning}, 
  booktitle = {Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)},
  month = {9},
  year={2023},
  pages = {126--130},
  %abstract = {Deep learning has been widely used recently for sound event detection and classification. Its success is linked to the availability of sufficiently large datasets, possibly with corresponding annotations when supervised learning is considered. In bioacoustic applications, most tasks come with few labelled training data, because annotating long recordings is time consuming and costly. Therefore supervised learning is not the best suited approach to solve bioacoustic tasks. The bioacoustic community recasted the problem of sound event detection within the framework of few-shot learning, i.e. training a system with only few labeled examples. The few-shot bioacoustic sound event detection task in the DCASE challenge focuses on detecting events in long audio recordings given only five annotated examples for each class of interest. In this paper, we show that learning a rich feature extractor from scratch can be achieved by leveraging data augmentation using a supervised contrastive learning framework. We highlight the ability of this framework to transfer well for five-shot event detection on previously unseen classes in the training data. We obtain an F-score of 63.46% on the validation set and 42.7% on the test set, ranking second in the DCASE challenge. We provide an ablation study for the critical choices of data augmentation techniques as well as for the learning strategy applied on the training set. Our code is available on Github.},
  %eprint={2309.00878},
  %archivePrefix={arXiv},
  %primaryClass={cs.SD},
}

% cow stuff
@inproceedings{Vidana-Vila2023,
  author = {Vidaña-Vila, Ester and Malé, Jordi and Freixes, Marc and Solís-Cifré, Mireia and Jiménez, Miquel and Larrondo, Cristian and Guevara, Raúl and Miranda, Joana and Duboc, Leticia and Mainau, Eva and Llonch, Pol and Alsina-Pagès, Rosa Ma},
  title = {Automatic Detection of Cow Vocalizations Using Convolutional Neural Networks},
  booktitle = {Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)},
  month = {9},
  year = {2023},
  pages = {206--210},
  %address = {Tampere, Finland},
  %abstract = {The well-being of animals holds significant importance in our society. Apart from the ethical concerns, recent studies have highlighted the correlation of animal growth, reproductive potential, and overall productivity with animal welfare. In this context, the vocalizations of cows have emerged as a valuable indicator of their well-being for veterinary researchers, but gathering and labelling the vocalizations for their in-depth study is time-consuming and labour-intensive. For this reason, in this work, we present an acoustic event detection algorithm that has been trained and validated with different setups using acoustic data collected from two different farms. The experimental set-up consists of a Convolutional Neural Network followed by a post-processing stage for the detection of vocalizations, so veterinary researchers can easily analyze them. The experimental evaluation assesses the importance of selecting the convenient post-processing and overlapping acoustic window for finding new vocalizations. Furthermore, the study evaluates the significance of using data collected specifically from the same farm for acoustic event detection, as opposed to employing data from a different farm. Results show that by merging training data from different farms, including the farm that is being evaluated, an F1 score of 57.40\% and a recall of 74.05\% can be achieved.},
}

% fewshot learning, dcase
@article{Nolasco2023,
  author = {Ines Nolasco and Shubhr Singh and Veronica Morfi and Vincent Lostanlen and Ariana Strandburg-Peshkin and Ester Vidaña-Vila and Lisa Gill and Hanna Pamuła and Helen Whitehead and Ivan Kiskin and Frants H. Jensen and Joe Morford and Michael G. Emmerson and Elisabetta Versace and Emily Grout and Haohe Liu and Burooj Ghani and Dan Stowell},
  title = {Learning to detect an animal sound from five examples},
  journal = {Ecological Informatics},
  volume = {77},
  pages = {102258},
  year = {2023},
  issn = {1574-9541},
  %doi = {https://doi.org/10.1016/j.ecoinf.2023.102258},
  %url = {https://www.sciencedirect.com/science/article/pii/S157495412300287X},
  %keywords = {Bioacoustics, Deep learning, Event detection, Few-shot learning},
  %abstract = {Automatic detection and classification of animal sounds has many applications in biodiversity monitoring and animal behavior. In the past twenty years, the volume of digitised wildlife sound available has massively increased, and automatic classification through deep learning now shows strong results. However, bioacoustics is not a single task but a vast range of small-scale tasks (such as individual ID, call type, emotional indication) with wide variety in data characteristics, and most bioacoustic tasks do not come with strongly-labelled training data. The standard paradigm of supervised learning, focussed on a single large-scale dataset and/or a generic pre-trained algorithm, is insufficient. In this work we recast bioacoustic sound event detection within the AI framework of few-shot learning. We adapt this framework to sound event detection, such that a system can be given the annotated start/end times of as few as 5 events, and can then detect events in long-duration audio—even when the sound category was not known at the time of algorithm training. We introduce a collection of open datasets designed to strongly test a system's ability to perform few-shot sound event detections, and we present the results of a public contest to address the task. Our analysis shows that prototypical networks are a very common used strategy and they perform well when enhanced with adaptations for general characteristics of animal sounds. However, systems with high time resolution capabilities perform the best in this challenge. We demonstrate that widely-varying sound event durations are an important factor in performance, as well as non-stationarity, i.e. gradual changes in conditions throughout the duration of a recording. For fine-grained bioacoustic recognition tasks without massive annotated training data, our analysis demonstrate that few-shot sound event detection is a powerful new method, strongly outperforming traditional signal-processing detection methods in the fully automated scenario.}
}

% conservation
@article{Teixeira2019Bmoavbfc,
  author = {Daniella Teixeira and Martine Maron and Berndt J. van Rensburg},
  title = {Bioacoustic monitoring of animal vocal behavior for conservation},
  issue = {8},
  journal = {Conservation Science and Practice},
  volume = {1},
  year = {2019},
  %doi = {10.1111/csp2.72},
  %issn = {25784854},
  %abstract = {The popularity of bioacoustics for threatened species monitoring has surged. Large volumes of acoustic data can be collected autonomously and remotely with minimal human effort. The approach is commonly used to detect cryptic species and, more recently, to estimate abundance or density. However, the potential for conservation-relevant information to be derived from acoustic signatures associated with particular behavior is less well-exploited. Animal vocal behavior can reveal important information about critical life history events. In this study, we argue that the overlap of the disciplines of bioacoustics, vocal communication, and conservation behavior—thus, “acoustic conservation behavior”—has much to offer threatened species monitoring. In particular, vocalizations can serve as indicators of behavioral states and contexts that provide insight into populations as it relates to their conservation. We explore the information available from monitoring species' vocalizations that relate to reproduction and recruitment, alarm and defense, and social behavior, and how this information could translate into potential conservation benefits. While there are still challenges to processing acoustic data, we conclude that acoustic conservation behavior may improve threatened species monitoring where vocalizations reveal behaviors that are informative for management and decision-making.},
}

% cows, acoustic sensors, automated detection
@article{Shorten2023Asfad,
  author = {P.R. Shorten and L.B. Hunter},
  title = {Acoustic sensors for automated detection of cow vocalization duration and type},
  journal = {Computers and Electronics in Agriculture},
  volume = {208},
  pages = {107760},
  year = {2023},
  %issn = {0168-1699},
  %doi = {https://doi.org/10.1016/j.compag.2023.107760},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168169923001485},
  %keywords = {Cow, Acoustic, Machine learning, Neural network, Vocalization, Livestock},
  %abstract = {Acoustic technologies provide a non-invasive method to generate information about cow vocalization. This study demonstrated that collar attached acoustic sensors can differentiate between cow vocalization and background sounds obtained from cows under grazing conditions. The overall accuracy of the vocalization classification model was 99.5% in the test dataset, based on a total of 709 vocalization recordings from 10 cows. Vocalization samples were obtained from 10 trial cows, with a frequency that ranged from 3 to 452 vocalizations per cow. Algorithms were also developed to differentiate between three different cow vocalization classes (Open mouth, Closed mouth, Mixed mouth (Closed mouth followed by Open mouth)), with a model accuracy of 85% in the test dataset. Most cows demonstrated all three types of vocalization, and the between-cow variability in the probability of mixed vocalization per vocalization had a standard deviation of 0.13 relative an average probability of 0.74. The duration of the 709 individual vocalizations ranged from 0.88 to 3.37 s, with an average of 1.76 s and a standard deviation of 0.36 s. There was a between-cow variation in the duration of vocalization, with a standard deviation of 0.12 ± 0.04 s (P < 0.01). The performance of the model for the duration of vocalization had a coefficient of determination of R2 = 0.84 in the test dataset. Models to predict the proportion of a vocalization that is a closed vocalization had a coefficient of determination of R2 = 0.72 in the test dataset. The proportion of a mixed vocalization that is a closed vocalization ranged from 0.04 to 0.92 and had an average of 0.38 and a standard deviation of 0.15. There was also between-cow variation in the proportion of a mixed vocalization that is a closed vocalization, with a standard deviation of 0.08 ± 0.023 (P < 0.01). The mixed vocalization had an acoustic spectral and temporal pattern that was unique to the cow that generated the vocalization, and classification models for voice recognition had an accuracy of 80% in the test dataset. A prototype spectral unmixing algorithm was also developed to use the ensemble of cow-collar acoustic recordings from each cow to assign each cow vocalization to the cow that generated the vocalization. This study demonstrated that there is significant between-cow variability in cow vocalization traits, and that these traits can be determined using cow-attached acoustic sensors to provide information on the welfare and state of the animal.},
}

% animal welfare, vocalization
@article{Manteuffel2004Vofa,
  author = {Gerhard Manteuffel and Birger Puppe and Peter C Sch{\"o}n},
  title = {Vocalization of farm animals as a measure of welfare},
  journal = {Applied Animal Behaviour Science},
  volume = {88},
  number = {1},
  pages = {163-182},
  year = {2004},
  %issn = {0168-1591},
  %doi = {https://doi.org/10.1016/j.applanim.2004.02.012},
  %url = {https://www.sciencedirect.com/science/article/pii/S0168159104000565},
  keywords = {Vocalization, Farm animals, Pig, Cattle, Poultry, Animal welfare},
  abstract = {Emotionally relevant external events, hormone concentrations affecting mood and appetitive behaviour, thirst and hunger are able to stimulate a complex central nervous network that regulates endocrine feedback and behaviour in order to maintain or regain homeostasis. Particular states of mood or emotion may thus be accompanied by specific behaviours, vocalization being one of them. Hence, in farm animals vocalizations may supply us with hints on their well-being in an easy way, given that the meanings of the respective calls are well-established. Then, it is possible to judge acoustically uttered current needs and impaired welfare by non-invasive, continuous monitoring. Vocalizations may also modulate emotions of the receivers such that welfare may also be affected in conspecifics hearing distress utterances, e.g., in an abattoir. For these reasons, the analysis of farm animal vocalization has gained increasing interest in the last years and a variety of attempts to decode the meaning has been made. Concentrating on important farm animal species (pig, cattle, poultry) an overview of the present state-of-the-art in this discipline is given and present problems as well as possible future developments are discussed. Modern techniques of sound analysis have provided tools to discriminate, analyse and classify specific vocalizations. Taking advantage of this, future bioacoustical research for welfare assessment should focus on comprehensive studies of a broad spectrum of species specific distress vocalizations. Increasingly precise attributions of such utterances to environments, behavioural contexts and relevant physiological parameters will lead to a deeper understanding of their meaning and significance with respect to well-being of farm animals. The result will offer applicable acoustic tools for farming environments where non-invasive techniques for welfare judgements are urgently needed.}
}

% welfare survey
@misc{Mcloughlin2019,
  author = {Michael P. Mcloughlin and Rebecca Stewart and Alan G. McElligott},
  title = {Automated bioacoustics: Methods in ecology and conservation and their potential for animal welfare monitoring},
  issue = {155},
  journal = {Journal of the Royal Society Interface},
  volume = {16},
  year = {2019},
  %abstract = {Vocalizations carry emotional, physiological and individual information. This suggests that they may serve as potentially useful indicators for inferring animal welfare. At the same time, automated methods for analysing and classifying sound have developed rapidly, particularly in the fields of ecology, conservation and sound scene classification. These methods are already used to automatically classify animal vocalizations, for example, in identifying animal species and estimating numbers of individuals. Despite this potential, they have not yet found widespread application in animal welfare monitoring. In this review, we first discuss current trends in sound analysis for ecology, conservation and sound classification. Following this, we detail the vocalizations produced by three of the most important farm livestock species: chickens (Gallus gallus domesticus), pigs (Sus scrofa domesticus) and cattle (Bos taurus). Finally, we describe how these methods can be applied to monitor animal welfare with new potential for developing automated methods for large-scale farming.},
  %doi = {10.1098/rsif.2019.0225},
  %issn = {17425662},
}

% soundscape ecology
@article{Pijanowski2011SoundscapeEcology,
  author = {Bryan C. Pijanowski and Luis J. Villanueva-Rivera and Sarah L. Dumyahn and Almo Farina and Bernie L. Krause and Brian M. Napoletano and Stuart H. Gage and Nadia Pieretti},
  title = {Soundscape ecology: The science of sound in the landscape},
  issue = {3},
  journal = {BioScience},
  volume = {61},
  year = {2011},
  %abstract = {This article presents a unifying theory of soundscape ecology, which brings the idea of the soundscape the collection of sounds that emanate from landscapesinto a research and application focus. Our conceptual framework of soundscape ecology is based on the causes and consequences of biological (biophony), geophysical (geophony), and human-produced (anthrophony) sounds. We argue that soundscape ecology shares many parallels with landscape ecology, and it should therefore be considered a branch of this maturing field. We propose a research agenda for soundscape ecology that includes six areas: (1) measurement and analytical challenges, (2) spatial-temporal dynamics, (3) soundscape linkage to environmental covariates, (4) human impacts on the soundscape, (5) soundscape impacts on humans, and (6) soundscape impacts on ecosystems. We present case studies that illustrate different approaches to understanding soundscape dynamics. Because soundscapes are our auditory link to nature, we also argue for their protection, using the knowledge of how sounds are produced by the environment and humans. © 2011 by American Institute of Biological Sciences. All rights reserved.},
  %doi = {10.1525/bio.2011.61.3.6},
  %issn = {00063568},
}

% animal call
@article{Towsey2012,
  author = {Michael Towsey and Birgit Planitz and Alfredo Nantes and Jason Wimmer and Paul Roe},
  title = {A toolbox for animal call recognition},
  issue = {2},
  journal = {Bioacoustics},
  volume = {21},
  year = {2012},
  %doi = {10.1080/09524622.2011.648753},
  %issn = {09524622},
  %abstract = {Monitoring the natural environment is increasingly important as habit degradation and climate change reduce the world's biodiversity. We have developed software tools and applications to assist ecologists with the collection and analysis of acoustic data at large spatial and temporal scales. One of our key objectives is automated animal call recognition, and our approach has three novel attributes. First, we work with raw environmental audio, contaminated by noise and artefacts and containing calls that vary greatly in volume depending on the animal's proximity to the microphone. Second, initial experimentation suggested that no single recognizer could deal with the enormous variety of calls. Therefore, we developed a toolbox of generic recognizers to extract invariant features for each call type. Third, many species are cryptic and offer little data with which to train a recognizer. Many popular machine learning methods require large volumes of training and validation data and considerable time and expertise to prepare. Consequently we adopt bootstrap techniques that can be initiated with little data and refined subsequently. In this paper, we describe our recognition tools and present results for real ecological problems. © 2012 Taylor & Francis.},
}


% bird detection, survey
@inproceedings{Stowell2016Bdia,
  author = {Dan Stowell and Mike Wood and Yannis Stylianou and Herve Glotin},
  title = {Bird detection in audio: A survey and a challenge},
  booktitle = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
  volume = {2016-November},
  year = {2016},
  %abstract = {Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection.},
  %doi = {10.1109/MLSP.2016.7738875},
  %issn = {21610371},
}


% density estimation of animals
@article{Marques2013Eapdupa,
  author = {Tiago A. Marques and Len Thomas and Stephen W. Martin and David K. Mellinger and Jessica A. Ward and David J. Moretti and Danielle Harris and Peter L. Tyack},
  title = {Estimating animal population density using passive acoustics},
  issue = {2},
  journal = {Biological Reviews},
  volume = {88},
  year = {2013},
  %doi = {10.1111/brv.12001},
  %issn = {14647931},
  %abstract = {Reliable estimation of the size or density of wild animal populations is very important for effective wildlife management, conservation and ecology. Currently, the most widely used methods for obtaining such estimates involve either sighting animals from transect lines or some form of capture-recapture on marked or uniquely identifiable individuals. However, many species are difficult to sight, and cannot be easily marked or recaptured. Some of these species produce readily identifiable sounds, providing an opportunity to use passive acoustic data to estimate animal density. In addition, even for species for which other visually based methods are feasible, passive acoustic methods offer the potential for greater detection ranges in some environments (e.g. underwater or in dense forest), and hence potentially better precision. Automated data collection means that surveys can take place at times and in places where it would be too expensive or dangerous to send human observers. Here, we present an overview of animal density estimation using passive acoustic data, a relatively new and fast-developing field. We review the types of data and methodological approaches currently available to researchers and we provide a framework for acoustics-based density estimation, illustrated with examples from real-world case studies. We mention moving sensor platforms (e.g. towed acoustics), but then focus on methods involving sensors at fixed locations, particularly hydrophones to survey marine mammals, as acoustic-based density estimation research to date has been concentrated in this area. Primary among these are methods based on distance sampling and spatially explicit capture-recapture. The methods are also applicable to other aquatic and terrestrial sound-producing taxa. We conclude that, despite being in its infancy, density estimation based on passive acoustic data likely will become an important method for surveying a number of diverse taxa, such as sea mammals, fish, birds, amphibians, and insects, especially in situations where inferences are required over long periods of time. There is considerable work ahead, with several potentially fruitful research areas, including the development of (i) hardware and software for data acquisition, (ii) efficient, calibrated, automated detection and classification systems, and (iii) statistical approaches optimized for this application. Further, survey design will need to be developed, and research is needed on the acoustic behaviour of target species. Fundamental research on vocalization rates and group sizes, and the relation between these and other factors such as season or behaviour state, is critical. Evaluation of the methods under known density scenarios will be important for empirically validating the approaches presented here. © 2012 Cambridge Philosophical Society.},
}

% soundscape, biodiversity
@article{Sethi2022Spsoitf,
  author = {Sarab S. Sethi and Robert M. Ewers and Nick S. Jones and Jani Sleutel and Adi Shabrani and Nursyamin Zulkifli and Lorenzo Picinali},
  title = {Soundscapes predict species occurrence in tropical forests},
  journal = {Oikos},
  volume = {2022},
  year = {2022},
  issue = {3},
  %doi = {10.1111/oik.08525},
  %issn = {16000706},
  %abstract = {Accurate occurrence data is necessary for the conservation of keystone or endangered species, but acquiring it is usually slow, laborious and costly. Automated acoustic monitoring offers a scalable alternative to manual surveys but identifying species vocalisations requires large manually annotated training datasets, and is not always possible (e.g. for lesser studied or silent species). A new approach is needed that rapidly predicts species occurrence using smaller and more coarsely labelled audio datasets. We investigated whether local soundscapes could be used to infer the presence of 32 avifaunal and seven herpetofaunal species in 20 min recordings across a tropical forest degradation gradient in Sabah, Malaysia. Using acoustic features derived from a convolutional neural network (CNN), we characterised species indicative soundscapes by training our models on a temporally coarse labelled point-count dataset. Soundscapes successfully predicted the occurrence of 34 out of the 39 species across the two taxonomic groups, with area under the curve (AUC) metrics from 0.53 up to 0.87. The highest accuracies were achieved for species with strong temporal occurrence patterns. Soundscapes were a better predictor of species occurrence than above-ground carbon density – a metric often used to quantify habitat quality across forest degradation gradients. Our results demonstrate that soundscapes can be used to efficiently predict the occurrence of a wide variety of species and provide a new direction for data driven large-scale assessments of habitat suitability.},
}

% monitoring network,
@article{Sethi2020SA,
  author = {Sethi, Sarab S. and Ewers, Robert M. and Jones, Nick S. and Signorelli, Aaron and Picinali, Lorenzo and Orme, Christopher David L.},
  title = {SAFE Acoustics: An open-source, real-time eco-acoustic monitoring network in the tropical rainforests of Borneo},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {10},
  pages = {1182-1185},
  year = {2020},
  %keywords = {applied ecology, bioinformatics, community ecology, conservation, monitoring (community ecology), monitoring (population ecology), software, surveys},
  %doi = {https://doi.org/10.1111/2041-210X.13438},
  %url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13438},
  %eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13438},
  %abstract = {Abstract Automated monitoring approaches offer an avenue to unlocking large-scale insight into how ecosystems respond to human pressures. However, since data collection and data analyses are often treated independently, there are currently no open-source examples of end-to-end, real-time ecological monitoring networks. Here, we present the complete implementation of an autonomous acoustic monitoring network deployed in the tropical rainforests of Borneo. Real-time audio is uploaded remotely from the field, indexed by a central database, and delivered via an API to a public-facing website. We provide the open-source code and design of our monitoring devices, the central web2py database, and the ReactJS website. Furthermore, we demonstrate an extension of this infrastructure to deliver real-time analyses of the eco-acoustic data. By detailing a fully functional, open source, and extensively tested design, our work will accelerate the rate at which fully autonomous monitoring networks mature from technological curiosities, and towards genuinely impactful tools in ecology.},
}

% monitoring, autonomous, hardware
@article{Sethi2018AutonomousMonitoring,
  author = {Sethi, Sarab S. and Ewers, Robert M. and Jones, Nick S. and Orme, Christopher David L. and Picinali, Lorenzo},
  title = {Robust, real-time and autonomous monitoring of ecosystems with an open, low-cost, networked device},
  journal = {Methods in Ecology and Evolution},
  volume = {9},
  number = {12},
  pages = {2383-2387},
  year = {2018},
  %keywords = {acoustic monitoring, autonomous monitoring, biodiversity monitoring, ecosystem monitoring, open-source software, real-time monitoring},
  %doi = {https://doi.org/10.1111/2041-210X.13089},
  %url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13089},
  %eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13089},
  %abstract = {Abstract Automated methods of monitoring ecosystems provide a cost-effective way to track changes in natural system's dynamics across temporal and spatial scales. However, methods of recording and storing data captured from the field still require significant manual effort. Here, we introduce an open source, inexpensive, fully autonomous ecosystem monitoring unit for capturing and remotely transmitting continuous data streams from field sites over long time-periods. We provide a modular software framework for deploying various sensors, together with implementations to demonstrate proof of concept for continuous audio monitoring and time-lapse photography. We show how our system can outperform comparable technologies for fractions of the cost, provided a local mobile network link is available. The system is robust to unreliable network signals and has been shown to function in extreme environmental conditions, such as in the tropical rainforests of Sabah, Borneo. We provide full details on how to assemble the hardware, and the open-source software. Paired with appropriate automated analysis techniques, this system could provide spatially dense, near real-time, continuous insights into ecosystem and biodiversity dynamics at a low cost.},
}

% schlüter, bird
@inproceedings{Schlter2018BirdIF,
  author={Jan Schl{\"u}ter},
  title={Bird Identification from Timestamped, Geotagged Audio Recordings},
  booktitle={Conference and Labs of the Evaluation Forum},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:51941747},
}

% arbimon
@article{Aide2013Arbimon,
  author = {T. Mitchell Aide and Carlos Corrada-Bravo and Marconi Campos-Cerqueira and Carlos Milan and Giovany Vega and Rafae Alvarez},
  title = {Real-time bioacoustics monitoring and automated species identification},
  abstract = {Traditionally, animal species diversity and abundance is assessed using a variety of methods that are generally costly, limited in space and time, and most importantly, they rarely include a permanent record. Given the urgency of climate change and the loss of habitat, it is vital that we use new technologies to improve and expand global biodiversity monitoring to thousands of sites around the world. In this article, we describe the acoustical component of the Automated Remote BiodiversityMonitoring Network (ARBIMON), a novel combination of hardware and software for automating data acquisition, data management, and species identification based on audio recordings. The major components of the cyberinfrastructure include: a solar powered remote monitoring station that sends 1-min recordings every 10 min to a base station, which relays the recordings in real-time to the project server, where the recordings are processed and uploaded to the project website (arbimon.net). Along with a module for viewing, listening, and annotating recordings, the website includes a species identification interface to help users create machine learning algorithms to automate species identification. To demonstrate the system we present data on the vocal activity patterns of birds, frogs, insects, and mammals from Puerto Rico and Costa Rica. © 2013 Aide et al.},
  doi = {10.7717/peerj.103},
  issn = {21678359},
  issue = {1},
  journal = {PeerJ},
  volume = {2013},
  year = {2013},
}

% bioacoustic ai
@online{BioacousticAi,
  author = {{Naturalis Biodiversity Center}},
  title = {BioAcoustic AI},
  year = 2023,
  url = {https://bioacousticai.eu/},
  urldate = {2024-01-30}
}

% dcase
@book{DCASE2023Workshop,
  author = {Fuentes, Magdalena and Heittola, Toni and Imoto, Keisuke and Mesaros, Annamaria and Politis, Archontis and Serizel, Romain and Virtanen, Tuomas},
  title = {Proceedings of the 8th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2023)},
  year = {2023},
  publisher = {Tampere University},
  isbn = {978-952-03-3171-9},
  month = {9},
  address = {Tampere, Finland},
}

% cow stuff
@inproceedings{Vidana-Vila2023,
  author = {Vidaña-Vila, Ester and Malé, Jordi and Freixes, Marc and Solís-Cifré, Mireia and Jiménez, Miquel and Larrondo, Cristian and Guevara, Raúl and Miranda, Joana and Duboc, Leticia and Mainau, Eva and Llonch, Pol and Alsina-Pagès, Rosa Ma},
  title = {Automatic Detection of Cow Vocalizations Using Convolutional Neural Networks},
  booktitle = {Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)},
  address = {Tampere, Finland},
  month = {9},
  year = {2023},
  pages = {206--210},
  abstract = {The well-being of animals holds significant importance in our society. Apart from the ethical concerns, recent studies have highlighted the correlation of animal growth, reproductive potential, and overall productivity with animal welfare. In this context, the vocalizations of cows have emerged as a valuable indicator of their well-being for veterinary researchers, but gathering and labelling the vocalizations for their in-depth study is time-consuming and labour-intensive. For this reason, in this work, we present an acoustic event detection algorithm that has been trained and validated with different setups using acoustic data collected from two different farms. The experimental set-up consists of a Convolutional Neural Network followed by a post-processing stage for the detection of vocalizations, so veterinary researchers can easily analyze them. The experimental evaluation assesses the importance of selecting the convenient post-processing and overlapping acoustic window for finding new vocalizations. Furthermore, the study evaluates the significance of using data collected specifically from the same farm for acoustic event detection, as opposed to employing data from a different farm. Results show that by merging training data from different farms, including the farm that is being evaluated, an F1 score of 57.40\% and a recall of 74.05\% can be achieved.},
}

% bioacoustic roadmap
@article{Stowell2022,
  author = {Dan Stowell},
  title = {Computational bioacoustics with deep learning: a review and roadmap},
  journal = {CoRR},
  volume = {abs/2112.06725},
  yea = {2021},
  url = {https://arxiv.org/abs/2112.06725},
  eprinttype = {arXiv},
  eprint = {2112.06725},
  timestamp = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-2112-06725.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}

% birdnet
@article{Kahl2021,
  author = {Kahl, Stefan and Wood, Connor M. and Eibl, Maximilian and Klinck, Holger},
  title = {{BirdNET: A deep learning solution for avian diversity monitoring}},
  abstract = {Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286 h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670 h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4 years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation.},
  doi = {10.1016/j.ecoinf.2021.101236},
  issn = {15749541},
  journal = {Ecological Informatics},
  volume = {61},
  year = {2021},
}

% bioacoustic monitoring
@article{DeCamargo2019,
  author = {de Camargo, Ulisses and Roslin, Tomas and Ovaskainen, Otso},
  title = {{Spatio-temporal scaling of biodiversity in acoustic tropical bird communities}},
  abstract = {Automated analysis of acoustic communities is a rapidly emerging approach for the characterization and monitoring of biodiversity. To evaluate its utility, we should verify that such ‘bioacoustics' can accurately detect ecological signal in spatiotemporal acoustic data. Targeting the ‘Biological Dynamics of Forest Fragments Project' sites in Brazil, we ask: What is the relative contribution of the spatial, temporal and habitat dimension to variation in bird acoustic communities in a previously fragmented tropical rainforest? Does the functional diversity of bird communities scale similarly to space and time as does species diversity, when both are recorded by bioacoustics means? Overall, is the imprint of landscape fragmentation 30 years ago still audible in the present-day soundscape? We sampled forty-four sites in secondary forest and 107 sites in old-growth forest, resulting in 11 000 h of audio recordings. We detected 60 bird species with satisfactory precision and recovered a linear log–log relation between sampling time and species diversity. Sites in primary forest host more species than sites in secondary forest, but the difference decreased with sampling time, as the slope was slightly higher in secondary than primary forests. Functional diversity, as exposed by vocalizing birds, accumulates faster than does species diversity. The similarity among local communities decreases with distance in both time and space, but stability in time is remarkably high: two acoustic samples from the same site one year (or more) apart prove more similar than two samples taken at the same time but from sites situated just a few hundred meters apart. These findings suggest that habitat modification can be heard as a long-lasting imprint on the soundscape of regenerating habitats and identify soundscape–area and soundscape–time relations as a promising tool for biodiversity research, applied biomonitoring and restoration ecology.},
  doi = {10.1111/ecog.04544},
  issn = {16000587},
  journal = {Ecography},
  number = {11},
  volume = {42},
  year = {2019},
}