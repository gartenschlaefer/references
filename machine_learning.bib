% --
% machine learning

% todo, speaker recognition
@INPROCEEDINGS{Vaessen2022Wav2VecSpeaker,
  author={Vaessen, Nik and Van Leeuwen, David A.},
  title={Fine-Tuning Wav2Vec2 for Speaker Recognition}, 
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  year={2022},
  volume={},
  %number={},
  %pages={7967-7971},
  %keywords={Codes;Additives;Conferences;Speech recognition;Signal processing;Acoustics;Speaker recognition;speaker recognition;wav2vec2;transfer learning},
  %doi={10.1109/ICASSP43922.2022.9746952},
}

% dimension reduction, survey, methods
@article{Ayesha2020DimensionRed,
  author = {Shaeela Ayesha and Muhammad Kashif Hanif and Ramzan Talib},
  title = {Overview and comparative study of dimensionality reduction techniques for high dimensional data},
  journal = {Information Fusion},
  volume = {59},
  pages = {44-58},
  year = {2020},
  %issn = {1566-2535},
  %doi = {https://doi.org/10.1016/j.inffus.2020.01.005},
  %url = {https://www.sciencedirect.com/science/article/pii/S156625351930377X},
  %keywords = {Dimensionality reduction, Features, High dimensional data, Linear techniques, Nonlinear techniques},
  %abstract = {The recent developments in the modern data collection tools, techniques, and storage capabilities are leading towards huge volume of data. The dimensions of data indicate the number of features that have been measured for each observation. It has become a challenging task to analyze high dimensional data. Different dimensionality reduction techniques are available in literature to eliminate irrelevant and redundant features. Selection of an appropriate dimension reduction technique can help to enhance the processing speed and reduce the time and effort required to extract valuable information. This paper presents the state-of-the art dimensionality reduction techniques and their suitability for different types of data and application areas. Furthermore, the issues of dimensionality reduction techniques have been highlighted that can affect the accuracy and relevance of results.}
}

% neural network, acoustic
@article{Hinton2012DNNfAMiSR,
  author = {Geoffrey Hinton and Li Deng and Dong Yu and George E Dahl and Abdel-rahman Mohamed and Navdeep Jaitly and Andrew Senior and Vincent Vanhoucke and Patrick Nguyen and Tara N Sainath and Brian Kingsbury},
  title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition},
  journal = {IEEE Signal Processing Magazine},
  issue = {November},
  year = {2012},
  %issn = {1053-5888},
  %doi = {10.1109/MSP.2012.2205597},
  %abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
}